{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5061740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Callable, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd9617c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>blast_furnace_slag</th>\n",
       "      <th>fly_ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarse_aggregate</th>\n",
       "      <th>fine_aggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>concrete_compressive_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "      <td>31.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "      <td>23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "      <td>32.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "      <td>32.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   blast_furnace_slag   fly_ash   water   superplasticizer  \\\n",
       "0      540.0                  0.0       0.0   162.0                2.5   \n",
       "1      540.0                  0.0       0.0   162.0                2.5   \n",
       "2      332.5                142.5       0.0   228.0                0.0   \n",
       "3      332.5                142.5       0.0   228.0                0.0   \n",
       "4      198.6                132.4       0.0   192.0                0.0   \n",
       "...      ...                  ...       ...     ...                ...   \n",
       "1025   276.4                116.0      90.3   179.6                8.9   \n",
       "1026   322.2                  0.0     115.6   196.0               10.4   \n",
       "1027   148.5                139.4     108.6   192.7                6.1   \n",
       "1028   159.1                186.7       0.0   175.6               11.3   \n",
       "1029   260.9                100.5      78.3   200.6                8.6   \n",
       "\n",
       "       coarse_aggregate   fine_aggregate   age   concrete_compressive_strength  \n",
       "0                1040.0            676.0    28                           79.99  \n",
       "1                1055.0            676.0    28                           61.89  \n",
       "2                 932.0            594.0   270                           40.27  \n",
       "3                 932.0            594.0   365                           41.05  \n",
       "4                 978.4            825.5   360                           44.30  \n",
       "...                 ...              ...   ...                             ...  \n",
       "1025              870.1            768.3    28                           44.28  \n",
       "1026              817.9            813.4    28                           31.18  \n",
       "1027              892.4            780.0    28                           23.70  \n",
       "1028              989.6            788.9    28                           32.77  \n",
       "1029              864.5            761.5    28                           32.40  \n",
       "\n",
       "[1030 rows x 9 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the concrete data\n",
    "df = pd.read_csv(\"data/concrete_data.csv\")\n",
    "\n",
    "# Display the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82141b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cement', ' blast_furnace_slag', ' fly_ash', ' water',\n",
       "       ' superplasticizer', ' coarse_aggregate', ' fine_aggregate', ' age',\n",
       "       ' concrete_compressive_strength'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate into train and test datasets\n",
    "\n",
    "train_df = df.sample(frac=0.7, random_state=42) # random state ensures we always get the same sample\n",
    "test_df = df.drop(train_df.index)\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a7c9c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.89, 44.3 , 42.33, 47.81, 41.84, 28.24, 52.12, 41.72, 53.69,\n",
       "       38.41, 50.46, 40.76, 33.12, 50.95,  9.87, 48.7 , 33.4 , 28.6 ,\n",
       "       24.4 , 35.3 , 49.2 , 55.6 , 54.1 , 56.1 , 68.3 , 66.9 , 60.29,\n",
       "       68.5 , 71.3 , 74.7 , 71.3 , 49.9 , 60.2 , 64.3 , 55.2 , 66.1 ,\n",
       "       72.99, 79.4 , 77.3 , 59.89, 62.5 , 57.6 , 67.8 , 24.89, 29.45,\n",
       "       10.38, 22.84, 33.96, 21.06, 26.4 , 35.34, 20.92, 24.9 , 28.47,\n",
       "       38.56, 10.76,  7.75, 30.39, 50.77, 53.9 , 22.32, 24.54, 31.35,\n",
       "       40.86, 30.23, 29.22, 38.33, 42.92, 44.4 , 45.08, 15.44, 26.77,\n",
       "       45.84, 29.65, 13.12, 36.64, 45.37, 48.67, 23.51, 39.15, 55.64,\n",
       "       52.04, 33.36, 44.14, 42.29, 42.22, 56.85, 21.91, 56.74, 33.73,\n",
       "       46.64, 50.08, 66.95, 52.2 , 46.23, 31.97, 43.06, 67.57, 41.37,\n",
       "       60.28, 56.83, 51.02, 44.13, 55.65, 47.28, 29.16, 67.87, 58.52,\n",
       "       53.58, 14.4 , 21.29, 15.82, 12.55,  8.49, 11.98, 19.42, 41.41,\n",
       "       27.22, 39.64, 51.26, 55.02, 49.99, 53.66, 56.06, 33.56, 57.03,\n",
       "       44.42, 53.39, 35.36, 25.02, 54.77, 22.75, 25.51, 36.84, 45.9 ,\n",
       "       61.46, 55.45, 33.49, 29.55, 37.92, 32.01, 72.1 , 39.  , 13.4 ,\n",
       "       59.49, 24.4 , 31.97, 27.74, 25.42, 27.94, 32.63, 25.75, 33.08,\n",
       "       24.07, 21.82, 21.07, 14.84, 32.05, 39.7 , 38.7 ,  7.51, 18.2 ,\n",
       "       37.44, 27.04, 18.42, 21.95, 24.1 , 25.08, 25.97, 27.63, 12.54,\n",
       "       27.53,  7.84, 30.57, 26.06, 43.7 , 30.14, 12.73, 20.87, 19.54,\n",
       "       47.71, 43.38, 29.89, 24.29, 29.23, 10.39, 27.87, 20.42, 13.57,\n",
       "       15.75, 24.28, 36.59, 14.14, 23.52,  6.81, 41.68,  9.56, 50.53,\n",
       "       17.17, 32.1 , 36.96, 43.57, 35.76, 38.7 , 14.31, 17.44, 37.91,\n",
       "       33.61, 40.86, 18.91, 30.96, 54.28, 14.5 , 26.06, 33.21, 44.09,\n",
       "       52.61,  6.27, 27.92, 39.  , 41.24, 14.99, 13.52, 24.  , 22.44,\n",
       "       21.16, 25.18, 21.65, 29.39, 41.05, 26.74, 61.92, 47.22, 51.04,\n",
       "       41.64, 19.69, 39.58, 48.79, 37.42, 34.68, 33.8 , 42.42, 55.94,\n",
       "       58.78, 20.77, 25.18, 16.5 , 15.42, 27.68, 26.86, 45.3 , 52.82,\n",
       "       42.64, 40.87, 33.3 , 38.46, 35.23, 32.76, 32.4 , 18.28, 31.42,\n",
       "       31.03, 25.56, 36.44, 32.96, 40.68, 19.01,  8.54, 32.24, 40.93,\n",
       "       23.79, 23.74, 35.86, 28.99,  9.74, 33.8 , 39.84, 27.23, 30.65,\n",
       "       15.34, 23.89, 22.93, 28.63, 28.94, 40.93, 17.96, 19.01,  8.54,\n",
       "       32.25, 23.52, 29.73, 52.45, 23.79, 46.23, 43.58, 35.87, 29.07,\n",
       "       33.42, 39.06, 15.57, 44.61, 52.83, 61.24, 33.31, 37.27, 41.54,\n",
       "       39.46, 37.92, 31.18])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy the target column from the input data into separate numpy arrays\n",
    "\n",
    "train_targets = train_df[' concrete_compressive_strength'].to_numpy()\n",
    "test_targets = test_df[' concrete_compressive_strength'].to_numpy()\n",
    "\n",
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba74abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 540. ,    0. ,    0. , ..., 1055. ,  676. ,   28. ],\n",
       "       [ 198.6,  132.4,    0. , ...,  978.4,  825.5,  360. ],\n",
       "       [ 190. ,  190. ,    0. , ...,  932. ,  670. ,   90. ],\n",
       "       ...,\n",
       "       [ 159.8,  250. ,    0. , ..., 1049.3,  688.2,   28. ],\n",
       "       [ 166. ,  259.7,    0. , ...,  858.8,  826.8,   28. ],\n",
       "       [ 322.2,    0. ,  115.6, ...,  817.9,  813.4,   28. ]],\n",
       "      shape=(309, 8))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the target column from the input data to create input feature arrays\n",
    "\n",
    "train_features = train_df.drop(columns=[' concrete_compressive_strength']).to_numpy()\n",
    "test_features = test_df.drop(columns=[' concrete_compressive_strength']).to_numpy()\n",
    "\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "919ab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_absolute_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21094448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.isVectorizable = False\n",
    "\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"Forward method not implemented.\")\n",
    "\n",
    "    def randomize(self, weight_scale, bias_scale) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "364255a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-input_data))\n",
    "\n",
    "class ActivationReLU(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "class ActivationTanh(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5f8cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a layer class for the MLP\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, size_input: int, size_hidden: int):\n",
    "        \"\"\"Initialize with weights and biases.\"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden = size_hidden\n",
    "        self.weights = self.init_weights()\n",
    "        self.bias = self.init_biases()\n",
    "        self.isVectorizable = True\n",
    "\n",
    "    def init_weights(self, weight_scale=0.1):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        return np.random.randn(self.size_hidden, self.size_input) * weight_scale\n",
    "\n",
    "    def init_biases(self, bias_scale=0.001):\n",
    "        \"\"\"Initialize biases.\"\"\"\n",
    "        return np.full((self.size_hidden, 1), bias_scale)\n",
    "\n",
    "    def randomize(self, weight_scale=0.1, bias_scale=0.001):\n",
    "        \"\"\"Randomize weights and biases.\"\"\"\n",
    "        self.weights = self.init_weights(weight_scale=weight_scale)\n",
    "        self.bias = self.init_biases(bias_scale=bias_scale)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.dot(self.weights, X) + self.bias\n",
    "\n",
    "    def to_vector(self) -> np.ndarray:\n",
    "        \"\"\"Flatten weights and biases into a single vector.\"\"\"\n",
    "        return np.concatenate((self.weights.flatten(), self.bias.flatten()))\n",
    "\n",
    "    def from_vector(self, vector: np.ndarray) -> int:\n",
    "        \"\"\"Set weights and biases from a single vector.\"\"\"\n",
    "        self.weights = vector[:self.weights.size].reshape(self.weights.shape)\n",
    "        self.bias = vector[self.weights.size:].reshape(self.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8067fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bias:\n",
      "[[0.001]\n",
      " [0.001]\n",
      " [0.001]]\n",
      "weights:\n",
      "[[-0.04005844 -0.09732415  0.02133211 -0.06674535  0.02244217]\n",
      " [ 0.02763738  0.20551104 -0.05428025  0.1110359  -0.18428898]\n",
      " [ 0.00886421 -0.07824237 -0.03074459 -0.05056569 -0.04701805]]\n",
      "input:\n",
      "[[ 2.37212528]\n",
      " [ 1.16974849]\n",
      " [-0.02513111]\n",
      " [ 0.1910399 ]\n",
      " [ 2.27754507]]\n",
      "output:\n",
      "[[-0.17004247]\n",
      " [-0.0901945 ]\n",
      " [-0.18547004]]\n",
      "\n",
      "vector:\n",
      "[-0.04005844 -0.09732415  0.02133211 -0.06674535  0.02244217  0.02763738\n",
      "  0.20551104 -0.05428025  0.1110359  -0.18428898  0.00886421 -0.07824237\n",
      " -0.03074459 -0.05056569 -0.04701805  0.001       0.001       0.001     ]\n",
      "layer2 bias:\n",
      "[[0.001]\n",
      " [0.001]\n",
      " [0.001]]\n",
      "layer2 weights:\n",
      "[[-0.04005844 -0.09732415  0.02133211 -0.06674535  0.02244217]\n",
      " [ 0.02763738  0.20551104 -0.05428025  0.1110359  -0.18428898]\n",
      " [ 0.00886421 -0.07824237 -0.03074459 -0.05056569 -0.04701805]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the Linear class\n",
    "layer = Linear(size_input=5, size_hidden=3)\n",
    "X_sample = np.random.randn(1, 5).T\n",
    "output = layer.forward(X_sample)\n",
    "\n",
    "vector = layer.to_vector()\n",
    "layer2 = Linear(size_input=5, size_hidden=3)\n",
    "layer2.from_vector(vector)\n",
    "\n",
    "print(f\"\"\"\n",
    "bias:\n",
    "{layer.bias}\n",
    "weights:\n",
    "{layer.weights}\n",
    "input:\n",
    "{X_sample}\n",
    "output:\n",
    "{output}\n",
    "\n",
    "vector:\n",
    "{vector}\n",
    "layer2 bias:\n",
    "{layer2.bias}\n",
    "layer2 weights:\n",
    "{layer2.weights}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec4dc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers: Layer, randomize: bool = True):\n",
    "        self.layers = layers\n",
    "        self.vectorizable_layers: List[Linear] = [layer for layer in self.layers if layer.isVectorizable]\n",
    "        self.vector_indexes = []\n",
    "        index = 0\n",
    "        for layer in self.vectorizable_layers:\n",
    "            size_layer_params = layer.weights.size + layer.bias.size\n",
    "            self.vector_indexes.append((index, index + size_layer_params))\n",
    "            index += size_layer_params\n",
    "\n",
    "        if randomize:\n",
    "            self.randomize()\n",
    "\n",
    "    def randomize(self, weight_scale=0.1, bias_scale=0.001):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize(weight_scale=weight_scale, bias_scale=bias_scale)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def to_vector(self) -> np.ndarray:\n",
    "        \"\"\"Concatenate parameters from all layers into a single vector.\"\"\"\n",
    "        param_vector = np.array([])\n",
    "        for layer in self.vectorizable_layers:\n",
    "            param_vector = np.concatenate((param_vector, layer.to_vector()))\n",
    "        return param_vector\n",
    "\n",
    "    def from_vector(self, param_vector: np.ndarray):\n",
    "        \"\"\"Set parameters from all layers from a single vector.\"\"\"\n",
    "        for i in range(len(self.vectorizable_layers)):\n",
    "            start_idx, end_idx = self.vector_indexes[i]\n",
    "            self.vectorizable_layers[i].from_vector(param_vector[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a6628e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input:\n",
      "[[ 0.90210987]\n",
      " [-1.13551437]\n",
      " [ 0.45592557]\n",
      " [-0.69345151]\n",
      " [-0.73155188]]\n",
      "output:\n",
      "[[0.04056956]]\n",
      "\n",
      "vector:\n",
      "[ 0.10833489  0.06089958 -0.01027794 -0.05735469  0.07323356  0.13261914\n",
      "  0.2199454  -0.03282382  0.02624438 -0.21840896 -0.06449131 -0.17071431\n",
      "  0.10549269  0.01660184 -0.02536641  0.001       0.001       0.001\n",
      " -0.1375311   0.09268533  0.02587397  0.12118568 -0.04006506 -0.03949557\n",
      "  0.04782748  0.18589993  0.1828015  -0.0193258   0.0611696  -0.08570413\n",
      "  0.001       0.001       0.001       0.001       0.03562279  0.09599868\n",
      " -0.05987689  0.00873039  0.001     ]\n",
      "\n",
      "\n",
      "initial model2 output:\n",
      "[[-0.08052072]]\n",
      "\n",
      "model2 output after from_vector:\n",
      "[[0.04056956]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the Sequential class\n",
    "\n",
    "mlp = Sequential(\n",
    "    Linear(size_input=5, size_hidden=3),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=3, size_hidden=4),\n",
    "    ActivationSigmoid(),\n",
    "    Linear(size_input=4, size_hidden=1)\n",
    ")\n",
    "X_sample = np.random.randn(1, 5).T\n",
    "output = mlp.forward(X_sample)\n",
    "vector = mlp.to_vector()\n",
    "print(f\"\"\"\n",
    "input:\n",
    "{X_sample}\n",
    "output:\n",
    "{output}\n",
    "\n",
    "vector:\n",
    "{vector}\n",
    "\"\"\")\n",
    "\n",
    "mlp.randomize()\n",
    "print(f\"\"\"\n",
    "initial model2 output:\n",
    "{mlp.forward(X_sample)}\n",
    "\"\"\")\n",
    "\n",
    "mlp.from_vector(vector)\n",
    "print(f\"\"\"model2 output after from_vector:\n",
    "{mlp.forward(X_sample)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc7987b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of AI class for training the MLP with our PSO\n",
    "\n",
    "@dataclass\n",
    "class AccelerationCoefficients:\n",
    "    inertia_weight: float\n",
    "    cognitive_weight: float\n",
    "    social_weight: float\n",
    "    global_best_weight: float\n",
    "    jump_size: float\n",
    "    max_velocity: float\n",
    "    max_position: float\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, position: np.ndarray, accel_coeff: AccelerationCoefficients, fitness: float):\n",
    "        self.accel_coeff = accel_coeff\n",
    "        # Initialize other attributes like position, velocity, personal best, etc.\n",
    "        self.position = position\n",
    "        self.velocity = np.random.randn(position.shape[0]) * 0.1\n",
    "        self.fittest = fitness\n",
    "        self.informants: List[Particle] = []\n",
    "\n",
    "        self.best_personal: np.ndarray = position.copy()\n",
    "\n",
    "    def get_best_informant(self):\n",
    "        informant_fittest = None\n",
    "        best_informant = None\n",
    "        for informant in self.informants:\n",
    "            if best_informant is None or informant.fittest < informant_fittest:\n",
    "                informant_fittest = informant.fittest\n",
    "                best_informant = informant\n",
    "        return best_informant.position\n",
    "\n",
    "    def update_velocity(self, best_global):\n",
    "        best_informant = self.get_best_informant()\n",
    "        for i in range(len(self.position)):\n",
    "            b = np.random.random() * self.accel_coeff.cognitive_weight\n",
    "            c = np.random.random() * self.accel_coeff.social_weight\n",
    "            d = np.random.random() * self.accel_coeff.global_best_weight\n",
    "            inertia = self.accel_coeff.inertia_weight * self.velocity[i]\n",
    "            velocity_cognitive = b * (self.best_personal[i] - self.position[i])\n",
    "            velocity_social = c * (best_informant[i] - self.position[i])\n",
    "            velocity_global = d * (best_global[i] - self.position[i])\n",
    "            new_velocity = inertia + velocity_cognitive + velocity_social + velocity_global\n",
    "            self.velocity[i] = np.clip(new_velocity, -self.accel_coeff.max_velocity, self.accel_coeff.max_velocity)\n",
    "\n",
    "    def update_position(self):\n",
    "        self.position += self.velocity * self.accel_coeff.jump_size\n",
    "        self.position = np.clip(self.position, -self.accel_coeff.max_position, self.accel_coeff.max_position)\n",
    "\n",
    "class ParticleSwarmOptimisation:\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.ndarray[tuple[int, int]],\n",
    "            Y: np.ndarray[tuple[int]],\n",
    "            swarm_size: int,\n",
    "            accel_coeff: AccelerationCoefficients,\n",
    "            num_informants: int,\n",
    "            loss_function,\n",
    "            particle_initial_position_scale: Tuple[float, float],\n",
    "            model: Sequential,\n",
    "        ):\n",
    "        self.accel_coeff = accel_coeff\n",
    "        self.swarm_size = swarm_size\n",
    "        self.num_informants = num_informants\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.model = model\n",
    "\n",
    "        self.losses = []\n",
    "        self.avg_fitnesses = []\n",
    "\n",
    "        self.population: List[Particle] = []\n",
    "        for _ in range(swarm_size):\n",
    "            self.model.randomize(weight_scale=particle_initial_position_scale[0], bias_scale=particle_initial_position_scale[1])\n",
    "            particle_fitness = self.loss_function(self.Y, self.model.forward(self.X))\n",
    "            self.population.append(Particle(position=self.model.to_vector(), accel_coeff=accel_coeff, fitness=particle_fitness))\n",
    "\n",
    "        self.best_global: np.ndarray = self.population[0].position.copy()\n",
    "        self.best_global_fitness: float = self.population[0].fittest\n",
    "\n",
    "    def update_informants(self):\n",
    "        if self.num_informants >= self.swarm_size:\n",
    "            raise ValueError(\"Number of informants must be less than swarm size.\")\n",
    "        for particle in self.population:\n",
    "            others = [p for p in self.population if p is not particle]\n",
    "            particle.informants = np.random.choice(others, size=self.num_informants, replace=False)\n",
    "\n",
    "    def update_best_global(self):\n",
    "        loss = 0.0\n",
    "        fitnesses = []\n",
    "        for particle in self.population:\n",
    "            self.model.from_vector(particle.position)\n",
    "            fitness = self.loss_function(self.Y, self.model.forward(self.X))\n",
    "            fitnesses.append(fitness)\n",
    "            loss += fitness\n",
    "            if fitness < particle.fittest:\n",
    "                particle.best_personal = particle.position.copy()\n",
    "                particle.fittest = fitness\n",
    "                if self.best_global_fitness is None or fitness < self.best_global_fitness:\n",
    "                    self.best_global = particle.position.copy()\n",
    "                    self.best_global_fitness = fitness\n",
    "        return np.mean(fitnesses)\n",
    "\n",
    "    def get_accuracy(self, x, y_true) -> float:\n",
    "        \"\"\"Evaluate the accuracy in percent of the best global model on given data.\"\"\"\n",
    "        self.model.from_vector(self.best_global)\n",
    "        y_pred = self.model.forward(x)\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        accuracy = 100 * (1.0 - mae / np.mean(np.abs(y_true)))\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def update_velocities(self):\n",
    "        for particle in self.population:\n",
    "            particle.update_velocity(self.best_global)\n",
    "\n",
    "    def update_positions(self):\n",
    "        for particle in self.population:\n",
    "            particle.update_position()\n",
    "\n",
    "    def plot(self, epoch, avg_fitness):\n",
    "        if epoch % 10 == 0:\n",
    "            self.avg_fitnesses.append(avg_fitness)\n",
    "            self.losses.append(self.best_global_fitness)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.losses, label=\"Loss\")\n",
    "            ax.plot(\n",
    "                self.avg_fitnesses,\n",
    "                label=\"Average Fitness\",\n",
    "                linestyle=\"--\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Training Loss\")\n",
    "            ax.legend()\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "    def train_epoch(self):\n",
    "        avg_fitness = self.update_best_global()\n",
    "        self.update_velocities()\n",
    "        self.update_positions()\n",
    "        return avg_fitness\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.update_informants()\n",
    "        for epoch in range(epoch):\n",
    "            avg_fitness = self.train_epoch()\n",
    "            self.plot(epoch, avg_fitness)\n",
    "        return (self.best_global, self.best_global_fitness, self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0a41068",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ParticleSwarmOptimisation.__init__() got an unexpected keyword argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m particle_initial_position_scale = (\u001b[32m0.0001\u001b[39m, \u001b[32m0.087\u001b[39m)\n\u001b[32m     24\u001b[39m loss_function = mean_squared_error\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m pso = \u001b[43mParticleSwarmOptimisation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccel_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccel_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m (final_position, final_score, losses) = pso.train()\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal particle fitness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: ParticleSwarmOptimisation.__init__() got an unexpected keyword argument 'epochs'"
     ]
    }
   ],
   "source": [
    "mlp = Sequential(\n",
    "    Linear(size_input=train_features.shape[1], size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=1),\n",
    ")\n",
    "\n",
    "predictions = mlp.forward(test_features.T)\n",
    "\n",
    "swarm_size = 30\n",
    "epochs = 100\n",
    "accel_coeff = AccelerationCoefficients(\n",
    "        inertia_weight=0.68,\n",
    "        cognitive_weight=2.80,\n",
    "        social_weight=0.88,\n",
    "        global_best_weight=0.96,\n",
    "        jump_size=0.6,\n",
    "        max_velocity=0.9,\n",
    "        max_position=3.87,\n",
    "    )\n",
    "num_informants = 4\n",
    "particle_initial_position_scale = (0.0001, 0.087)\n",
    "loss_function = mean_squared_error\n",
    "\n",
    "pso = ParticleSwarmOptimisation(\n",
    "    X=train_features.T,\n",
    "    Y=train_targets,\n",
    "    swarm_size=swarm_size,\n",
    "    epochs=epochs,\n",
    "    accel_coeff=accel_coeff,\n",
    "    num_informants=num_informants,\n",
    "    loss_function=loss_function,\n",
    "    particle_initial_position_scale=particle_initial_position_scale,\n",
    "    model=mlp\n",
    ")\n",
    "\n",
    "(final_position, final_score, losses) = pso.train()\n",
    "print(f\"Final particle fitness: {final_score}\")\n",
    "print(f\"Final particle position sample: {final_position[:5]}\")\n",
    "mlp.from_vector(final_position)\n",
    "predictions = mlp.forward(test_features.T)\n",
    "\n",
    "plt.scatter(test_targets, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Predictions vs True Values\")\n",
    "plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', lw=2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy = pso.get_accuracy(train_features.T, train_targets)\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "test_accuracy = pso.get_accuracy(test_features.T, test_targets)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Params:\n",
    "model: MLP with layers {[type(layer).__name__ for layer in mlp.layers]}\n",
    "swarm_size: {swarm_size}\n",
    "epochs: {epochs}\n",
    "accel_coeff: {accel_coeff}\n",
    "num_informants: {num_informants}\n",
    "loss_function: {loss_function.__name__}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final loss\", losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1128db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ParticleSwarmOptimisation.__init__() got an unexpected keyword argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m accuracies = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tentatives):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     pso = \u001b[43mParticleSwarmOptimisation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccel_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccel_coeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     (final_position, final_score, losses) = pso.train()\n\u001b[32m     43\u001b[39m     accuracy = pso.get_accuracy(test_features.T, test_targets)\n",
      "\u001b[31mTypeError\u001b[39m: ParticleSwarmOptimisation.__init__() got an unexpected keyword argument 'epochs'"
     ]
    }
   ],
   "source": [
    "# Evaluate fitness of a Pso tentatives times\n",
    "mlp = Sequential(\n",
    "    Linear(size_input=train_features.shape[1], size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=1),\n",
    ")\n",
    "\n",
    "swarm_size = 20\n",
    "epochs = 30\n",
    "accel_coeff = AccelerationCoefficients(\n",
    "        inertia_weight=0.68,\n",
    "        cognitive_weight=2.80,\n",
    "        social_weight=0.88,\n",
    "        global_best_weight=0.96,\n",
    "        jump_size=0.6,\n",
    "        max_velocity=0.9,\n",
    "        max_position=3.87,\n",
    "    )\n",
    "num_informants = 4\n",
    "particle_initial_position_scale = (0.0001, 0.087)\n",
    "loss_function = mean_squared_error\n",
    "\n",
    "tentatives = 100\n",
    "\n",
    "final_scores = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(tentatives):\n",
    "    pso = ParticleSwarmOptimisation(\n",
    "        X=train_features.T,\n",
    "        Y=train_targets,\n",
    "        swarm_size=swarm_size,\n",
    "        epochs=epochs,\n",
    "        accel_coeff=accel_coeff,\n",
    "        num_informants=num_informants,\n",
    "        loss_function=loss_function,\n",
    "        particle_initial_position_scale=particle_initial_position_scale,\n",
    "        model=mlp\n",
    "    )\n",
    "    (final_position, final_score, losses) = pso.train()\n",
    "    accuracy = pso.get_accuracy(test_features.T, test_targets)\n",
    "    print(f\"Run {i+1}: Final particle fitness: {final_score}\")\n",
    "    final_scores.append(final_score)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "# Histogram that shows accuracy in x and pso that gives that accuracy in y\n",
    "plt.hist(accuracies, bins=10, edgecolor='black')\n",
    "plt.xlabel(\"Test Accuracy (%)\")\n",
    "plt.ylabel(\"Number of PSO runs\")\n",
    "plt.title(\"Distribution of Test Accuracies over PSO Runs\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bec4cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class PsoGenome:\n",
    "#     swarm_size: int\n",
    "#     accel_coeff: AccelerationCoefficients\n",
    "#     num_informants: int\n",
    "#     particle_initial_position_scale: Tuple[float, float]\n",
    "#     loss_function: callable\n",
    "\n",
    "#     def crossovers(self, other: 'PsoGenome') -> 'PsoGenome':\n",
    "#         # Implement crossover logic\n",
    "#         pass\n",
    "\n",
    "# class GeneticIndividual:\n",
    "#     def __init__(self, genome: np.ndarray):\n",
    "#         self.genome = genome\n",
    "#         self.fitness: float = float('inf')\n",
    "\n",
    "# class GeneticPsoOptimizer:\n",
    "#     def __init__(self, population_size: int, mutation_rate: float, crossover_rate: float, model: Sequential, loss_function):\n",
    "#         self.population_size = population_size\n",
    "#         self.mutation_rate = mutation_rate\n",
    "#         self.crossover_rate = crossover_rate\n",
    "#         self.loss_function = loss_function\n",
    "#         self.population: List[GeneticIndividual] = []\n",
    "#         self.fitnesses: List[float] = [self.evaluate_fitness(individual) for individual in self.population]\n",
    "\n",
    "#     def evaluate_fitness(self, individual: ParticleSwarmOptimisation) -> float:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ba05e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm for PSO Hyperparameter Optimization generated by ChatGPT\n",
    "# It is designed to find the best genome (hyperparameters of the PSO) because they are hard to fine tune by hand\n",
    "\n",
    "# --- Genome definition -----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class AccelerationCoefficientsGenome:\n",
    "    inertia_weight: float\n",
    "    cognitive_weight: float\n",
    "    social_weight: float\n",
    "    global_best_weight: float\n",
    "    jump_size: float\n",
    "    max_velocity: float\n",
    "    max_position: float\n",
    "\n",
    "    def mutate(self, mutation_rate: float, ranges: Dict[str, Tuple[float, float]]):\n",
    "        # gaussian perturbation per field with chance\n",
    "        for name in vars(self):\n",
    "            if random.random() < mutation_rate:\n",
    "                lo, hi = ranges[name]\n",
    "                cur = getattr(self, name)\n",
    "                # relative gaussian step\n",
    "                step = (hi - lo) * 0.1\n",
    "                new = cur + random.gauss(0, step)\n",
    "                setattr(self, name, float(np.clip(new, lo, hi)))\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover(a: 'AccelerationCoefficientsGenome', b: 'AccelerationCoefficientsGenome') -> 'AccelerationCoefficientsGenome':\n",
    "        # uniform crossover\n",
    "        out = {}\n",
    "        for name in vars(a):\n",
    "            out[name] = getattr(a, name) if random.random() < 0.5 else getattr(b, name)\n",
    "        return AccelerationCoefficientsGenome(**out)\n",
    "\n",
    "@dataclass\n",
    "class PsoGenome:\n",
    "    swarm_size: int\n",
    "    accel: AccelerationCoefficientsGenome\n",
    "    num_informants: int\n",
    "    particle_initial_position_scale: Tuple[float, float]\n",
    "    # optionally include model architecture: list of hidden layer sizes and activation names\n",
    "    ann_layers: Optional[Tuple[int, ...]] = None\n",
    "    ann_activation: str = \"tanh\"\n",
    "    # keep a pointer/identifier to the loss function if desired (not serializable)\n",
    "    # loss_function: Callable = field(default=None, repr=False)\n",
    "\n",
    "    def copy(self) -> 'PsoGenome':\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def mutate(self, mutation_rate: float, bounds: dict):\n",
    "        # mutate swarm size (discrete), num_informants (discrete), scales and accel\n",
    "        if random.random() < mutation_rate:\n",
    "            self.swarm_size = int(np.clip(self.swarm_size + random.randint(-4, 4), bounds['swarm_size'][0], bounds['swarm_size'][1]))\n",
    "        if random.random() < mutation_rate:\n",
    "            self.num_informants = int(np.clip(self.num_informants + random.randint(-2, 2), 1, max(1, self.swarm_size - 1)))\n",
    "        if random.random() < mutation_rate:\n",
    "            s0, s1 = self.particle_initial_position_scale\n",
    "            s0 += random.gauss(0, (bounds['position_scale'][1] - bounds['position_scale'][0]) * 0.05)\n",
    "            s1 += random.gauss(0, (bounds['bias_scale'][1] - bounds['bias_scale'][0]) * 0.05)\n",
    "            self.particle_initial_position_scale = (float(np.clip(s0, bounds['position_scale'][0], bounds['position_scale'][1])),\n",
    "                                                   float(np.clip(s1, bounds['bias_scale'][0], bounds['bias_scale'][1])))\n",
    "        # mutate acceleration coeffs\n",
    "        self.accel.mutate(mutation_rate, bounds['accel_ranges'])\n",
    "        # mutate architecture (small chance)\n",
    "        if self.ann_layers is not None and random.random() < mutation_rate:\n",
    "            layers = list(self.ann_layers)\n",
    "            if random.random() < 0.5 and len(layers) > 0:\n",
    "                # tweak a layer size\n",
    "                idx = random.randrange(len(layers))\n",
    "                layers[idx] = int(np.clip(layers[idx] + random.randint(-8, 8), 1, 1024))\n",
    "            else:\n",
    "                # either add or remove layer\n",
    "                if random.random() < 0.5 and len(layers) > 1:\n",
    "                    layers.pop(random.randrange(len(layers)))\n",
    "                else:\n",
    "                    # add a small layer\n",
    "                    insert_at = random.randrange(len(layers)+1)\n",
    "                    layers.insert(insert_at, random.randint(1, 32))\n",
    "            self.ann_layers = tuple(layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover(a: 'PsoGenome', b: 'PsoGenome', crossover_rate: float = 0.5) -> 'PsoGenome':\n",
    "        # single-point for architecture, uniform for many numeric\n",
    "        child = a.copy()\n",
    "        # swarm size: average with some chance\n",
    "        child.swarm_size = int((a.swarm_size if random.random() < 0.5 else b.swarm_size))\n",
    "        child.num_informants = int((a.num_informants if random.random() < 0.5 else b.num_informants))\n",
    "        child.particle_initial_position_scale = (a.particle_initial_position_scale if random.random() < 0.5 else b.particle_initial_position_scale)\n",
    "        child.accel = AccelerationCoefficientsGenome.crossover(a.accel, b.accel)\n",
    "        # architecture crossover (if both defined)\n",
    "        if a.ann_layers and b.ann_layers:\n",
    "            if random.random() < crossover_rate:\n",
    "                # one-point crossover on layer lists\n",
    "                la, lb = list(a.ann_layers), list(b.ann_layers)\n",
    "                cut_a = random.randrange(len(la))\n",
    "                cut_b = random.randrange(len(lb))\n",
    "                new_layers = tuple(la[:cut_a] + lb[cut_b:])\n",
    "                child.ann_layers = new_layers\n",
    "        else:\n",
    "            child.ann_layers = a.ann_layers or b.ann_layers\n",
    "        return child\n",
    "\n",
    "# --- Evaluator -------------------------------------------------------------\n",
    "\n",
    "class PsoEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        Y_test: np.ndarray,\n",
    "        base_model_builder: Callable[[PsoGenome], Any],\n",
    "        loss_function: Callable[[np.ndarray, np.ndarray], float],\n",
    "        max_train_seconds: float = 10.0,\n",
    "        patience_window: int = 20,\n",
    "        num_genome_repeats_per_iteration: int = 3,\n",
    "        max_repeats_per_genome: int = 30,\n",
    "        explosion_factor: float = 1e6,\n",
    "        accuracy_checks_every: int = 10,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        base_model_builder: function(genome)->model where model implements the interface expected by ParticleSwarmOptimisation:\n",
    "            - randomize(weight_scale, bias_scale)\n",
    "            - to_vector()/from_vector()\n",
    "            - forward(X)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.base_model_builder = base_model_builder\n",
    "        self.loss_function = loss_function\n",
    "        self.max_train_seconds = max_train_seconds\n",
    "        self.patience_window = patience_window\n",
    "        self.num_genome_repeats_per_iteration = num_genome_repeats_per_iteration\n",
    "        self.explosion_factor = explosion_factor\n",
    "        self.accuracy_checks_every = accuracy_checks_every\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def evaluate(self, genome: PsoGenome) -> float:\n",
    "        \"\"\"\n",
    "        Returns scalar fitness. Lower is better.\n",
    "        Implements:\n",
    "         - time-limited training\n",
    "         - early stopping on recent-window no-improvement (penalise)\n",
    "         - explosion detection (penalise)\n",
    "        \"\"\"\n",
    "        key = str(genome)\n",
    "        if key in self.cache and self.cache[key]['repeats'] >= self.num_genome_repeats_per_iteration:\n",
    "            return self.cache[key]['acc']\n",
    "\n",
    "        # build model\n",
    "        model = self.base_model_builder(genome)\n",
    "\n",
    "        # build PSO with the genome's params\n",
    "        accel = genome.accel\n",
    "        accel_obj = AccelerationCoefficients(\n",
    "            inertia_weight=accel.inertia_weight,\n",
    "            cognitive_weight=accel.cognitive_weight,\n",
    "            social_weight=accel.social_weight,\n",
    "            global_best_weight=accel.global_best_weight,\n",
    "            jump_size=accel.jump_size,\n",
    "            max_velocity=accel.max_velocity,\n",
    "            max_position=accel.max_position\n",
    "        )\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        for _ in range(self.num_genome_repeats_per_iteration):\n",
    "            pso = ParticleSwarmOptimisation(\n",
    "                X=self.X, Y=self.Y,\n",
    "                swarm_size=genome.swarm_size,\n",
    "                accel_coeff=accel_obj,\n",
    "                num_informants=max(1, min(genome.num_informants, genome.swarm_size - 1)),\n",
    "                loss_function=self.loss_function,\n",
    "                particle_initial_position_scale=genome.particle_initial_position_scale,\n",
    "                model=model\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "            last_losses = []\n",
    "            try:\n",
    "                epoch = 0\n",
    "                # replace PSO.train loop with a time-aware training\n",
    "                pso.update_informants()\n",
    "                # compute an initial loss to detect explosion (if available)\n",
    "                # We'll compute first fitness properly\n",
    "                initial_fitness = pso.update_best_global(0)\n",
    "                while True:\n",
    "                    # check time limit\n",
    "                    if time.time() - start_time > self.max_train_seconds:\n",
    "                        break\n",
    "                    # iterate a small PSO step: velocities, positions, recompute bests\n",
    "                    pso.update_velocities()\n",
    "                    pso.update_positions()\n",
    "                    avg_fitness = pso.update_best_global(epoch)\n",
    "                    last_losses.append(pso.best_global_fitness)\n",
    "\n",
    "                    # explosion detection\n",
    "                    if avg_fitness > initial_fitness * self.explosion_factor:\n",
    "                        # heavy penalty\n",
    "                        if self.verbose:\n",
    "                            print(\"[eval] explosion detected. stopping early.\")\n",
    "                        accuracies.append(0.0)\n",
    "\n",
    "                    if epoch % self.accuracy_checks_every == 0:\n",
    "                        acc = pso.get_accuracy(self.X_test, self.Y_test)\n",
    "                        accuracies.append(acc)\n",
    "                        if self.verbose:\n",
    "                            print(f\"[eval] epoch {epoch} best fitness {pso.best_global_fitness:.6g} avg fitness {avg_fitness:.6g} accuracy {acc:.2f}%\")\n",
    "                    # early stopping: check last window\n",
    "                    if len(last_losses) > self.patience_window:\n",
    "                        # consider improvement if best decreased at least once in window\n",
    "                        window = last_losses[-self.patience_window:]\n",
    "                        if max(window) < window[0]:  # no improvement\n",
    "                            if self.verbose:\n",
    "                                print(f\"[eval] early stopping at epoch {epoch}\")\n",
    "                            accuracies.append(pso.get_accuracy(self.X_test, self.Y_test))\n",
    "                            break\n",
    "                    epoch += 1\n",
    "                # normal return: the best found\n",
    "                if self.verbose:\n",
    "                    print(f\"[eval] completed training epochs: {epoch}, best fitness: {pso.best_global_fitness:.6g}\")\n",
    "                acc = pso.get_accuracy(self.X_test, self.Y_test)\n",
    "                accuracies.append(acc)\n",
    "            except Exception as e:\n",
    "                # crash in training -> penalize heavily\n",
    "                if self.verbose:\n",
    "                    print(\"[eval] Exception during PSO eval:\", e)\n",
    "                accuracies.append(0.0)\n",
    "            \n",
    "        if key in self.cache:\n",
    "            self.cache[key]['repeats'] += self.num_genome_repeats_per_iteration\n",
    "        else:\n",
    "            self.cache[key] = {'repeats': self.num_genome_repeats_per_iteration, 'accuracies': []}\n",
    "        updated_accuracies = self.cache[key]['accuracies'] + accuracies\n",
    "        mean_accuracy = np.mean(updated_accuracies)\n",
    "        self.cache[key]['acc'] = mean_accuracy\n",
    "\n",
    "        return mean_accuracy\n",
    "\n",
    "# --- Genetic algorithm ----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GeneticIndividual:\n",
    "    genome: PsoGenome\n",
    "    accuracy: float = float('inf')\n",
    "\n",
    "class GeneticPsoOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        evaluator: PsoEvaluator,\n",
    "        population_size: int = 20,\n",
    "        generations: int = 30,\n",
    "        mutation_rate: float = 0.1,\n",
    "        crossover_rate: float = 0.8,\n",
    "        elitism: int = 2,\n",
    "        tournament_k: int = 3,\n",
    "        parallel: bool = True,\n",
    "    ):\n",
    "        self.evaluator = evaluator\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.elitism = elitism\n",
    "        self.tournament_k = tournament_k\n",
    "        self.parallel = parallel\n",
    "        self.population: List[GeneticIndividual] = []\n",
    "\n",
    "    def initialize(self, seed_genome_factory: Callable[[], PsoGenome]):\n",
    "        self.population = [GeneticIndividual(seed_genome_factory()) for _ in range(self.population_size)]\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        # Evaluate all individuals (optionally in parallel)\n",
    "        if self.parallel:\n",
    "            with mp.Pool(max(1, mp.cpu_count() - 1)) as pool:\n",
    "                genomes = [ind.genome for ind in self.population]\n",
    "                results = pool.map(self.evaluator.evaluate, genomes)\n",
    "            for ind, f in zip(self.population, results):\n",
    "                ind.accuracy = f\n",
    "        else:\n",
    "            for ind in self.population:\n",
    "                ind.accuracy = self.evaluator.evaluate(ind.genome)\n",
    "\n",
    "    def tournament_select(self) -> PsoGenome:\n",
    "        contenders = random.sample(self.population, self.tournament_k)\n",
    "        best = max(contenders, key=lambda ind: ind.accuracy)\n",
    "        return best.genome.copy()\n",
    "\n",
    "    def step(self):\n",
    "        # create next generation\n",
    "        new_pop: List[GeneticIndividual] = []\n",
    "        # elitism: carry best individuals\n",
    "        sorted_pop = sorted(self.population, key=lambda ind: ind.accuracy, reverse=True)\n",
    "        for i in range(self.elitism):\n",
    "            new_pop.append(GeneticIndividual(sorted_pop[i].genome.copy(), sorted_pop[i].accuracy))\n",
    "        # fill rest\n",
    "        while len(new_pop) < self.population_size:\n",
    "            parent_a = self.tournament_select()\n",
    "            if random.random() < self.crossover_rate:\n",
    "                parent_b = self.tournament_select()\n",
    "                child_genome = PsoGenome.crossover(parent_a, parent_b, crossover_rate=self.crossover_rate)\n",
    "            else:\n",
    "                child_genome = parent_a.copy()\n",
    "            child_genome.mutate(self.mutation_rate, bounds=self._bounds())\n",
    "            new_pop.append(GeneticIndividual(child_genome))\n",
    "        self.population = new_pop\n",
    "\n",
    "    def run(self, seed_genome_factory: Callable[[], PsoGenome], verbose: bool = True):\n",
    "        self.initialize(seed_genome_factory)\n",
    "        self.evaluate_population()\n",
    "        best_history = []\n",
    "        for g in range(self.generations):\n",
    "            if verbose:\n",
    "                best = max(self.population, key=lambda ind: ind.accuracy)\n",
    "                avg_fitness = sum(ind.accuracy for ind in self.population) / len(self.population)\n",
    "                print(f\"[GA] gen {g:02d} best {best.accuracy:.6g} avg {avg_fitness:.6g}\")\n",
    "                with open(\"ga_pso_log.csv\", \"a\") as f:\n",
    "                    f.write(f\"{g},{best.accuracy},{avg_fitness}\\n\")\n",
    "            self.step()\n",
    "            self.evaluate_population()\n",
    "            best_history.append(min(self.population, key=lambda ind: ind.accuracy).accuracy)\n",
    "        # final best\n",
    "        best_ind = min(self.population, key=lambda ind: ind.accuracy)\n",
    "        return best_ind, best_history\n",
    "\n",
    "    def _bounds(self):\n",
    "        # central place for bounds used by mutate: feel free to expose\n",
    "        return {\n",
    "            'swarm_size': (4, 200),\n",
    "            'position_scale': (1e-4, 1.0),\n",
    "            'bias_scale': (1e-6, 1.0),\n",
    "            'accel_ranges': {\n",
    "                'inertia_weight': (0.0, 2.0),\n",
    "                'cognitive_weight': (0.0, 4.0),\n",
    "                'social_weight': (0.0, 4.0),\n",
    "                'global_best_weight': (0.0, 4.0),\n",
    "                'jump_size': (1e-4, 2.0),\n",
    "                'max_velocity': (1e-6, 10.0),\n",
    "                'max_position': (1e-3, 100.0),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca917c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GA] gen 00 best 0 avg 0\n",
      "[GA] gen 01 best 0 avg 0\n",
      "[GA] gen 02 best 0 avg 0\n",
      "[GA] gen 03 best 0 avg 0\n",
      "[GA] gen 04 best 0 avg 0\n",
      "[GA] gen 05 best 0 avg 0\n",
      "[GA] gen 06 best 0 avg 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage of GeneticPsoOptimizer\u001b[39;00m\n\u001b[32m      3\u001b[39m optimizer = GeneticPsoOptimizer(\n\u001b[32m      4\u001b[39m     evaluator=PsoEvaluator(\n\u001b[32m      5\u001b[39m         X=train_features.T,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     parallel=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_genome_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPsoGenome\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAccelerationCoefficientsGenome\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[43minertia_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcognitive_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m            \u001b[49m\u001b[43msocial_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m            \u001b[49m\u001b[43mglobal_best_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjump_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_velocity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mann_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mann_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Save output to a file\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgenetic_pso_optimizer_result.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 325\u001b[39m, in \u001b[36mGeneticPsoOptimizer.run\u001b[39m\u001b[34m(self, seed_genome_factory, verbose)\u001b[39m\n\u001b[32m    323\u001b[39m             f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest.accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_fitness\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m.step()\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     best_history.append(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.population, key=\u001b[38;5;28;01mlambda\u001b[39;00m ind: ind.accuracy).accuracy)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# final best\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 287\u001b[39m, in \u001b[36mGeneticPsoOptimizer.evaluate_population\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.population:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m         ind.accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mPsoEvaluator.evaluate\u001b[39m\u001b[34m(self, genome)\u001b[39m\n\u001b[32m    170\u001b[39m accuracies = []\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_genome_repeats_per_iteration):\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     pso = \u001b[43mParticleSwarmOptimisation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccel_coeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccel_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_informants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mswarm_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparticle_initial_position_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     start_time = time.time()\n\u001b[32m    184\u001b[39m     last_losses = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mParticleSwarmOptimisation.__init__\u001b[39m\u001b[34m(self, X, Y, swarm_size, accel_coeff, num_informants, loss_function, particle_initial_position_scale, model)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(swarm_size):\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.randomize(weight_scale=particle_initial_position_scale[\u001b[32m0\u001b[39m], bias_scale=particle_initial_position_scale[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     particle_fitness = \u001b[38;5;28mself\u001b[39m.loss_function(\u001b[38;5;28mself\u001b[39m.Y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m.population.append(Particle(position=\u001b[38;5;28mself\u001b[39m.model.to_vector(), accel_coeff=accel_coeff, fitness=particle_fitness))\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m.best_global: np.ndarray = \u001b[38;5;28mself\u001b[39m.population[\u001b[32m0\u001b[39m].position.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     20\u001b[39m output = X\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     output = layer.forward(output)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage of GeneticPsoOptimizer\n",
    "\n",
    "optimizer = GeneticPsoOptimizer(\n",
    "    evaluator=PsoEvaluator(\n",
    "        X=train_features.T,\n",
    "        Y=train_targets,\n",
    "        X_test=test_features.T,\n",
    "        Y_test=test_targets,\n",
    "        base_model_builder=lambda genome: Sequential(\n",
    "            *(\n",
    "                [Linear(size_input=train_features.shape[1], size_hidden=genome.ann_layers[0])] +\n",
    "                sum(\n",
    "                    ([ActivationReLU(), Linear(size_input=genome.ann_layers[i], size_hidden=genome.ann_layers[i+1])]\n",
    "                     for i in range(len(genome.ann_layers)-1)),\n",
    "                    []\n",
    "                ) +\n",
    "                [ActivationReLU(), Linear(size_input=genome.ann_layers[-1], size_hidden=1)]\n",
    "            ) if genome.ann_layers else\n",
    "            [\n",
    "                Linear(size_input=train_features.shape[1], size_hidden=32),\n",
    "                ActivationReLU(),\n",
    "                Linear(size_input=32, size_hidden=16),\n",
    "                ActivationReLU(),\n",
    "                Linear(size_input=16, size_hidden=1)\n",
    "            ]\n",
    "        ),\n",
    "        loss_function=mean_squared_error,\n",
    "        max_train_seconds=20.0,\n",
    "        patience_window=20,\n",
    "        num_genome_repeats_per_iteration=5,\n",
    "        max_repeats_per_genome=40,\n",
    "        explosion_factor=1e2,\n",
    "        accuracy_checks_every=10,\n",
    "        verbose=False\n",
    "    ),\n",
    "    population_size=20,\n",
    "    generations=50,\n",
    "    mutation_rate=0.2,\n",
    "    crossover_rate=0.7,\n",
    "    elitism=6,\n",
    "    tournament_k=3,\n",
    "    parallel=False\n",
    ")\n",
    "\n",
    "optimizer.run(\n",
    "    seed_genome_factory=lambda: PsoGenome(\n",
    "        swarm_size=random.randint(10, 200),\n",
    "        accel=AccelerationCoefficientsGenome(\n",
    "            inertia_weight=random.uniform(0.2, 0.9),\n",
    "            cognitive_weight=random.uniform(0.5, 3.0),\n",
    "            social_weight=random.uniform(0.2, 2.0),\n",
    "            global_best_weight=random.uniform(0.1, 2.0),\n",
    "            jump_size=random.uniform(0.01, 1.0),\n",
    "            max_velocity=random.uniform(0.001, 2.0),\n",
    "            max_position=random.uniform(0.1, 10.0)\n",
    "        ),\n",
    "        num_informants=random.randint(1, 10),\n",
    "        particle_initial_position_scale=(random.uniform(0.0001, 0.1), random.uniform(0.0001, 0.1)),\n",
    "        ann_layers=(32, 16),\n",
    "        ann_activation=\"relu\"\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save output to a file\n",
    "with open(\"genetic_pso_optimizer_result.txt\", \"w\") as f:\n",
    "    best_individual = max(optimizer.population, key=lambda ind: ind.accuracy)\n",
    "    f.write(f\"Best fitness: {best_individual.accuracy}\\n\")\n",
    "    f.write(f\"Best genome: {best_individual.genome}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e90e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
