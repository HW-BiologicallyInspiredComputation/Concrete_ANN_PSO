{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5061740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Callable, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9617c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concrete data\n",
    "df = pd.read_csv(\"data/concrete_data.csv\")\n",
    "\n",
    "# Display the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82141b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and test datasets\n",
    "\n",
    "train_df = df.sample(frac=0.7, random_state=42) # random state ensures we always get the same sample\n",
    "test_df = df.drop(train_df.index)\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the target column from the input data into separate numpy arrays\n",
    "\n",
    "train_targets = train_df[' concrete_compressive_strength'].to_numpy()\n",
    "test_targets = test_df[' concrete_compressive_strength'].to_numpy()\n",
    "\n",
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the target column from the input data to create input feature arrays\n",
    "\n",
    "train_features = train_df.drop(columns=[' concrete_compressive_strength']).to_numpy()\n",
    "test_features = test_df.drop(columns=[' concrete_compressive_strength']).to_numpy()\n",
    "\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_absolute_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21094448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.isVectorizable = False\n",
    "\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError(\"Forward method not implemented.\")\n",
    "\n",
    "    def randomize(self, weight_scale, bias_scale) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364255a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSigmoid(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-input_data))\n",
    "\n",
    "class ActivationReLU(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "class ActivationTanh(Layer):\n",
    "    def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a layer class for the MLP\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, size_input: int, size_hidden: int):\n",
    "        \"\"\"Initialize with weights and biases.\"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden = size_hidden\n",
    "        self.weights = self.init_weights()\n",
    "        self.bias = self.init_biases()\n",
    "        self.isVectorizable = True\n",
    "\n",
    "    def init_weights(self, weight_scale=0.1):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        return np.random.randn(self.size_hidden, self.size_input) * weight_scale\n",
    "\n",
    "    def init_biases(self, bias_scale=0.001):\n",
    "        \"\"\"Initialize biases.\"\"\"\n",
    "        return np.full((self.size_hidden, 1), bias_scale)\n",
    "\n",
    "    def randomize(self, weight_scale=0.1, bias_scale=0.001):\n",
    "        \"\"\"Randomize weights and biases.\"\"\"\n",
    "        self.weights = self.init_weights(weight_scale=weight_scale)\n",
    "        self.bias = self.init_biases(bias_scale=bias_scale)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.dot(self.weights, X) + self.bias\n",
    "\n",
    "    def to_vector(self) -> np.ndarray:\n",
    "        \"\"\"Flatten weights and biases into a single vector.\"\"\"\n",
    "        return np.concatenate((self.weights.flatten(), self.bias.flatten()))\n",
    "\n",
    "    def from_vector(self, vector: np.ndarray) -> int:\n",
    "        \"\"\"Set weights and biases from a single vector.\"\"\"\n",
    "        self.weights = vector[:self.weights.size].reshape(self.weights.shape)\n",
    "        self.bias = vector[self.weights.size:].reshape(self.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Linear class\n",
    "layer = Linear(size_input=5, size_hidden=3)\n",
    "X_sample = np.random.randn(1, 5).T\n",
    "output = layer.forward(X_sample)\n",
    "\n",
    "vector = layer.to_vector()\n",
    "layer2 = Linear(size_input=5, size_hidden=3)\n",
    "layer2.from_vector(vector)\n",
    "\n",
    "print(f\"\"\"\n",
    "bias:\n",
    "{layer.bias}\n",
    "weights:\n",
    "{layer.weights}\n",
    "input:\n",
    "{X_sample}\n",
    "output:\n",
    "{output}\n",
    "\n",
    "vector:\n",
    "{vector}\n",
    "layer2 bias:\n",
    "{layer2.bias}\n",
    "layer2 weights:\n",
    "{layer2.weights}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers: Layer, randomize: bool = True):\n",
    "        self.layers = layers\n",
    "        self.vectorizable_layers: List[Linear] = [layer for layer in self.layers if layer.isVectorizable]\n",
    "        self.vector_indexes = []\n",
    "        index = 0\n",
    "        for layer in self.vectorizable_layers:\n",
    "            size_layer_params = layer.weights.size + layer.bias.size\n",
    "            self.vector_indexes.append((index, index + size_layer_params))\n",
    "            index += size_layer_params\n",
    "\n",
    "        if randomize:\n",
    "            self.randomize()\n",
    "\n",
    "    def randomize(self, weight_scale=0.1, bias_scale=0.001):\n",
    "        for layer in self.layers:\n",
    "            layer.randomize(weight_scale=weight_scale, bias_scale=bias_scale)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def to_vector(self) -> np.ndarray:\n",
    "        \"\"\"Concatenate parameters from all layers into a single vector.\"\"\"\n",
    "        param_vector = np.array([])\n",
    "        for layer in self.vectorizable_layers:\n",
    "            param_vector = np.concatenate((param_vector, layer.to_vector()))\n",
    "        return param_vector\n",
    "\n",
    "    def from_vector(self, param_vector: np.ndarray):\n",
    "        \"\"\"Set parameters from all layers from a single vector.\"\"\"\n",
    "        for i in range(len(self.vectorizable_layers)):\n",
    "            start_idx, end_idx = self.vector_indexes[i]\n",
    "            self.vectorizable_layers[i].from_vector(param_vector[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6628e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Sequential class\n",
    "\n",
    "mlp = Sequential(\n",
    "    Linear(size_input=5, size_hidden=3),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=3, size_hidden=4),\n",
    "    ActivationSigmoid(),\n",
    "    Linear(size_input=4, size_hidden=1)\n",
    ")\n",
    "X_sample = np.random.randn(1, 5).T\n",
    "output = mlp.forward(X_sample)\n",
    "vector = mlp.to_vector()\n",
    "print(f\"\"\"\n",
    "input:\n",
    "{X_sample}\n",
    "output:\n",
    "{output}\n",
    "\n",
    "vector:\n",
    "{vector}\n",
    "\"\"\")\n",
    "\n",
    "mlp.randomize()\n",
    "print(f\"\"\"\n",
    "initial model2 output:\n",
    "{mlp.forward(X_sample)}\n",
    "\"\"\")\n",
    "\n",
    "mlp.from_vector(vector)\n",
    "print(f\"\"\"model2 output after from_vector:\n",
    "{mlp.forward(X_sample)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7987b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of AI class for training the MLP with our PSO\n",
    "\n",
    "@dataclass\n",
    "class AccelerationCoefficients:\n",
    "    inertia_weight: float\n",
    "    cognitive_weight: float\n",
    "    social_weight: float\n",
    "    global_best_weight: float\n",
    "    jump_size: float\n",
    "    max_velocity: float\n",
    "    max_position: float\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, position: np.ndarray, accel_coeff: AccelerationCoefficients, fitness: float):\n",
    "        self.accel_coeff = accel_coeff\n",
    "        # Initialize other attributes like position, velocity, personal best, etc.\n",
    "        self.position = position\n",
    "        self.velocity = np.random.randn(position.shape[0]) * 0.1\n",
    "        self.fittest = fitness\n",
    "        self.informants: List[Particle] = []\n",
    "\n",
    "        self.best_personal: np.ndarray = position.copy()\n",
    "\n",
    "    def get_best_informant(self):\n",
    "        informant_fittest = None\n",
    "        best_informant = None\n",
    "        for informant in self.informants:\n",
    "            if best_informant is None or informant.fittest < informant_fittest:\n",
    "                informant_fittest = informant.fittest\n",
    "                best_informant = informant\n",
    "        return best_informant.position\n",
    "\n",
    "    def update_velocity(self, best_global):\n",
    "        best_informant = self.get_best_informant()\n",
    "        for i in range(len(self.position)):\n",
    "            b = np.random.random() * self.accel_coeff.cognitive_weight\n",
    "            c = np.random.random() * self.accel_coeff.social_weight\n",
    "            d = np.random.random() * self.accel_coeff.global_best_weight\n",
    "            inertia = self.accel_coeff.inertia_weight * self.velocity[i]\n",
    "            velocity_cognitive = b * (self.best_personal[i] - self.position[i])\n",
    "            velocity_social = c * (best_informant[i] - self.position[i])\n",
    "            velocity_global = d * (best_global[i] - self.position[i])\n",
    "            new_velocity = inertia + velocity_cognitive + velocity_social + velocity_global\n",
    "            self.velocity[i] = np.clip(new_velocity, -self.accel_coeff.max_velocity, self.accel_coeff.max_velocity)\n",
    "\n",
    "    def update_position(self):\n",
    "        self.position += self.velocity * self.accel_coeff.jump_size\n",
    "        self.position = np.clip(self.position, -self.accel_coeff.max_position, self.accel_coeff.max_position)\n",
    "\n",
    "class ParticleSwarmOptimisation:\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.ndarray[tuple[int, int]],\n",
    "            Y: np.ndarray[tuple[int]],\n",
    "            swarm_size: int,\n",
    "            accel_coeff: AccelerationCoefficients,\n",
    "            num_informants: int,\n",
    "            loss_function,\n",
    "            particle_initial_position_scale: Tuple[float, float],\n",
    "            model: Sequential,\n",
    "        ):\n",
    "        self.accel_coeff = accel_coeff\n",
    "        self.swarm_size = swarm_size\n",
    "        self.num_informants = num_informants\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.model = model\n",
    "\n",
    "        self.losses = []\n",
    "        self.avg_fitnesses = []\n",
    "\n",
    "        self.population: List[Particle] = []\n",
    "        for _ in range(swarm_size):\n",
    "            self.model.randomize(weight_scale=particle_initial_position_scale[0], bias_scale=particle_initial_position_scale[1])\n",
    "            particle_fitness = self.loss_function(self.Y, self.model.forward(self.X))\n",
    "            self.population.append(Particle(position=self.model.to_vector(), accel_coeff=accel_coeff, fitness=particle_fitness))\n",
    "\n",
    "        self.best_global: np.ndarray = self.population[0].position.copy()\n",
    "        self.best_global_fitness: float = self.population[0].fittest\n",
    "\n",
    "    def update_informants(self):\n",
    "        if self.num_informants >= self.swarm_size:\n",
    "            raise ValueError(\"Number of informants must be less than swarm size.\")\n",
    "        for particle in self.population:\n",
    "            others = [p for p in self.population if p is not particle]\n",
    "            particle.informants = np.random.choice(others, size=self.num_informants, replace=False)\n",
    "\n",
    "    def update_best_global(self):\n",
    "        loss = 0.0\n",
    "        fitnesses = []\n",
    "        for particle in self.population:\n",
    "            self.model.from_vector(particle.position)\n",
    "            fitness = self.loss_function(self.Y, self.model.forward(self.X))\n",
    "            fitnesses.append(fitness)\n",
    "            loss += fitness\n",
    "            if fitness < particle.fittest:\n",
    "                particle.best_personal = particle.position.copy()\n",
    "                particle.fittest = fitness\n",
    "                if self.best_global_fitness is None or fitness < self.best_global_fitness:\n",
    "                    self.best_global = particle.position.copy()\n",
    "                    self.best_global_fitness = fitness\n",
    "        return np.mean(fitnesses)\n",
    "\n",
    "    def get_accuracy(self, x, y_true) -> float:\n",
    "        \"\"\"Evaluate the accuracy in percent of the best global model on given data.\"\"\"\n",
    "        self.model.from_vector(self.best_global)\n",
    "        y_pred = self.model.forward(x)\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        accuracy = 100 * (1.0 - mae / np.mean(np.abs(y_true)))\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def update_velocities(self):\n",
    "        for particle in self.population:\n",
    "            particle.update_velocity(self.best_global)\n",
    "\n",
    "    def update_positions(self):\n",
    "        for particle in self.population:\n",
    "            particle.update_position()\n",
    "\n",
    "    def plot(self, epoch, avg_fitness):\n",
    "        if epoch % 10 == 0:\n",
    "            self.avg_fitnesses.append(avg_fitness)\n",
    "            self.losses.append(self.best_global_fitness)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.losses, label=\"Loss\")\n",
    "            ax.plot(\n",
    "                self.avg_fitnesses,\n",
    "                label=\"Average Fitness\",\n",
    "                linestyle=\"--\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Training Loss\")\n",
    "            ax.legend()\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "    def train_epoch(self):\n",
    "        avg_fitness = self.update_best_global()\n",
    "        self.update_velocities()\n",
    "        self.update_positions()\n",
    "        return avg_fitness\n",
    "\n",
    "    def train(self, epochs):\n",
    "        self.update_informants()\n",
    "        for epoch in range(epochs):\n",
    "            avg_fitness = self.train_epoch()\n",
    "            self.plot(epoch, avg_fitness)\n",
    "        return (self.best_global, self.best_global_fitness, self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a41068",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential(\n",
    "    Linear(size_input=train_features.shape[1], size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=1),\n",
    ")\n",
    "\n",
    "predictions = mlp.forward(test_features.T)\n",
    "\n",
    "swarm_size = 30\n",
    "epochs = 100\n",
    "accel_coeff = AccelerationCoefficients(\n",
    "        inertia_weight=0.68,\n",
    "        cognitive_weight=2.80,\n",
    "        social_weight=0.88,\n",
    "        global_best_weight=0.96,\n",
    "        jump_size=0.6,\n",
    "        max_velocity=0.9,\n",
    "        max_position=3.87,\n",
    "    )\n",
    "num_informants = 4\n",
    "particle_initial_position_scale = (0.0001, 0.087)\n",
    "loss_function = mean_squared_error\n",
    "\n",
    "pso = ParticleSwarmOptimisation(\n",
    "    X=train_features.T,\n",
    "    Y=train_targets,\n",
    "    swarm_size=swarm_size,\n",
    "    accel_coeff=accel_coeff,\n",
    "    num_informants=num_informants,\n",
    "    loss_function=loss_function,\n",
    "    particle_initial_position_scale=particle_initial_position_scale,\n",
    "    model=mlp\n",
    ")\n",
    "\n",
    "(final_position, final_score, losses) = pso.train(epochs=epochs)\n",
    "print(f\"Final particle fitness: {final_score}\")\n",
    "print(f\"Final particle position sample: {final_position[:5]}\")\n",
    "mlp.from_vector(final_position)\n",
    "predictions = mlp.forward(test_features.T)\n",
    "\n",
    "plt.scatter(test_targets, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Predictions vs True Values\")\n",
    "plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', lw=2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy = pso.get_accuracy(train_features.T, train_targets)\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "test_accuracy = pso.get_accuracy(test_features.T, test_targets)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Params:\n",
    "model: MLP with layers {[type(layer).__name__ for layer in mlp.layers]}\n",
    "swarm_size: {swarm_size}\n",
    "epochs: {epochs}\n",
    "accel_coeff: {accel_coeff}\n",
    "num_informants: {num_informants}\n",
    "loss_function: {loss_function.__name__}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final loss\", losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1128db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fitness of a Pso tentatives times\n",
    "mlp = Sequential(\n",
    "    Linear(size_input=train_features.shape[1], size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=12),\n",
    "    ActivationReLU(),\n",
    "    Linear(size_input=12, size_hidden=1),\n",
    ")\n",
    "\n",
    "swarm_size = 20\n",
    "epochs = 30\n",
    "accel_coeff = AccelerationCoefficients(\n",
    "        inertia_weight=0.68,\n",
    "        cognitive_weight=2.80,\n",
    "        social_weight=0.88,\n",
    "        global_best_weight=0.96,\n",
    "        jump_size=0.6,\n",
    "        max_velocity=0.9,\n",
    "        max_position=3.87,\n",
    "    )\n",
    "num_informants = 4\n",
    "particle_initial_position_scale = (0.0001, 0.087)\n",
    "loss_function = mean_squared_error\n",
    "\n",
    "tentatives = 10\n",
    "\n",
    "final_scores = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(tentatives):\n",
    "    pso = ParticleSwarmOptimisation(\n",
    "        X=train_features.T,\n",
    "        Y=train_targets,\n",
    "        swarm_size=swarm_size,\n",
    "        accel_coeff=accel_coeff,\n",
    "        num_informants=num_informants,\n",
    "        loss_function=loss_function,\n",
    "        particle_initial_position_scale=particle_initial_position_scale,\n",
    "        model=mlp\n",
    "    )\n",
    "    (final_position, final_score, losses) = pso.train(epochs=epochs)\n",
    "    accuracy = pso.get_accuracy(test_features.T, test_targets)\n",
    "    print(f\"Run {i+1}: Final particle fitness: {final_score}\")\n",
    "    final_scores.append(final_score)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "# Histogram that shows accuracy in x and pso that gives that accuracy in y\n",
    "plt.hist(accuracies, bins=10, edgecolor='black')\n",
    "plt.xlabel(\"Test Accuracy (%)\")\n",
    "plt.ylabel(\"Number of PSO runs\")\n",
    "plt.title(\"Distribution of Test Accuracies over PSO Runs\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class PsoGenome:\n",
    "#     swarm_size: int\n",
    "#     accel_coeff: AccelerationCoefficients\n",
    "#     num_informants: int\n",
    "#     particle_initial_position_scale: Tuple[float, float]\n",
    "#     loss_function: callable\n",
    "\n",
    "#     def crossovers(self, other: 'PsoGenome') -> 'PsoGenome':\n",
    "#         # Implement crossover logic\n",
    "#         pass\n",
    "\n",
    "# class GeneticIndividual:\n",
    "#     def __init__(self, genome: np.ndarray):\n",
    "#         self.genome = genome\n",
    "#         self.fitness: float = float('inf')\n",
    "\n",
    "# class GeneticPsoOptimizer:\n",
    "#     def __init__(self, population_size: int, mutation_rate: float, crossover_rate: float, model: Sequential, loss_function):\n",
    "#         self.population_size = population_size\n",
    "#         self.mutation_rate = mutation_rate\n",
    "#         self.crossover_rate = crossover_rate\n",
    "#         self.loss_function = loss_function\n",
    "#         self.population: List[GeneticIndividual] = []\n",
    "#         self.fitnesses: List[float] = [self.evaluate_fitness(individual) for individual in self.population]\n",
    "\n",
    "#     def evaluate_fitness(self, individual: ParticleSwarmOptimisation) -> float:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba05e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm for PSO Hyperparameter Optimization generated by ChatGPT\n",
    "# It is designed to find the best genome (hyperparameters of the PSO) because they are hard to fine tune by hand\n",
    "\n",
    "# --- Genome definition -----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class AccelerationCoefficientsGenome:\n",
    "    inertia_weight: float\n",
    "    cognitive_weight: float\n",
    "    social_weight: float\n",
    "    global_best_weight: float\n",
    "    jump_size: float\n",
    "    max_velocity: float\n",
    "    max_position: float\n",
    "\n",
    "    def mutate(self, mutation_rate: float, ranges: Dict[str, Tuple[float, float]]):\n",
    "        # gaussian perturbation per field with chance\n",
    "        for name in vars(self):\n",
    "            if random.random() < mutation_rate:\n",
    "                lo, hi = ranges[name]\n",
    "                cur = getattr(self, name)\n",
    "                # relative gaussian step\n",
    "                step = (hi - lo) * 0.1\n",
    "                new = cur + random.gauss(0, step)\n",
    "                setattr(self, name, float(np.clip(new, lo, hi)))\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover(a: 'AccelerationCoefficientsGenome', b: 'AccelerationCoefficientsGenome') -> 'AccelerationCoefficientsGenome':\n",
    "        # uniform crossover\n",
    "        out = {}\n",
    "        for name in vars(a):\n",
    "            out[name] = getattr(a, name) if random.random() < 0.5 else getattr(b, name)\n",
    "        return AccelerationCoefficientsGenome(**out)\n",
    "\n",
    "@dataclass\n",
    "class PsoGenome:\n",
    "    swarm_size: int\n",
    "    accel: AccelerationCoefficientsGenome\n",
    "    num_informants: int\n",
    "    particle_initial_position_scale: Tuple[float, float]\n",
    "    # optionally include model architecture: list of hidden layer sizes and activation names\n",
    "    ann_layers: Optional[Tuple[int, ...]] = None\n",
    "    ann_activation: str = \"tanh\"\n",
    "    # keep a pointer/identifier to the loss function if desired (not serializable)\n",
    "    # loss_function: Callable = field(default=None, repr=False)\n",
    "\n",
    "    def copy(self) -> 'PsoGenome':\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def mutate(self, mutation_rate: float, bounds: dict):\n",
    "        # mutate swarm size (discrete), num_informants (discrete), scales and accel\n",
    "        if random.random() < mutation_rate:\n",
    "            self.swarm_size = int(np.clip(self.swarm_size + random.randint(-4, 4), bounds['swarm_size'][0], bounds['swarm_size'][1]))\n",
    "        if random.random() < mutation_rate:\n",
    "            self.num_informants = int(np.clip(self.num_informants + random.randint(-2, 2), 1, max(1, self.swarm_size - 1)))\n",
    "        if random.random() < mutation_rate:\n",
    "            s0, s1 = self.particle_initial_position_scale\n",
    "            s0 += random.gauss(0, (bounds['position_scale'][1] - bounds['position_scale'][0]) * 0.05)\n",
    "            s1 += random.gauss(0, (bounds['bias_scale'][1] - bounds['bias_scale'][0]) * 0.05)\n",
    "            self.particle_initial_position_scale = (float(np.clip(s0, bounds['position_scale'][0], bounds['position_scale'][1])),\n",
    "                                                   float(np.clip(s1, bounds['bias_scale'][0], bounds['bias_scale'][1])))\n",
    "        # mutate acceleration coeffs\n",
    "        self.accel.mutate(mutation_rate, bounds['accel_ranges'])\n",
    "        # mutate architecture (small chance)\n",
    "        if self.ann_layers is not None and random.random() < mutation_rate:\n",
    "            layers = list(self.ann_layers)\n",
    "            if random.random() < 0.5 and len(layers) > 0:\n",
    "                # tweak a layer size\n",
    "                idx = random.randrange(len(layers))\n",
    "                layers[idx] = int(np.clip(layers[idx] + random.randint(-8, 8), 1, 1024))\n",
    "            else:\n",
    "                # either add or remove layer\n",
    "                if random.random() < 0.5 and len(layers) > 1:\n",
    "                    layers.pop(random.randrange(len(layers)))\n",
    "                else:\n",
    "                    # add a small layer\n",
    "                    insert_at = random.randrange(len(layers)+1)\n",
    "                    layers.insert(insert_at, random.randint(1, 32))\n",
    "            self.ann_layers = tuple(layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover(a: 'PsoGenome', b: 'PsoGenome', crossover_rate: float = 0.5) -> 'PsoGenome':\n",
    "        # single-point for architecture, uniform for many numeric\n",
    "        child = a.copy()\n",
    "        # swarm size: average with some chance\n",
    "        child.swarm_size = int((a.swarm_size if random.random() < 0.5 else b.swarm_size))\n",
    "        child.num_informants = int((a.num_informants if random.random() < 0.5 else b.num_informants))\n",
    "        child.particle_initial_position_scale = (a.particle_initial_position_scale if random.random() < 0.5 else b.particle_initial_position_scale)\n",
    "        child.accel = AccelerationCoefficientsGenome.crossover(a.accel, b.accel)\n",
    "        # architecture crossover (if both defined)\n",
    "        if a.ann_layers and b.ann_layers:\n",
    "            if random.random() < crossover_rate:\n",
    "                # one-point crossover on layer lists\n",
    "                la, lb = list(a.ann_layers), list(b.ann_layers)\n",
    "                cut_a = random.randrange(len(la))\n",
    "                cut_b = random.randrange(len(lb))\n",
    "                new_layers = tuple(la[:cut_a] + lb[cut_b:])\n",
    "                child.ann_layers = new_layers\n",
    "        else:\n",
    "            child.ann_layers = a.ann_layers or b.ann_layers\n",
    "        return child\n",
    "\n",
    "# --- Evaluator -------------------------------------------------------------\n",
    "\n",
    "class PsoEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        Y_test: np.ndarray,\n",
    "        base_model_builder: Callable[[PsoGenome], Any],\n",
    "        loss_function: Callable[[np.ndarray, np.ndarray], float],\n",
    "        max_train_seconds: float = 10.0,\n",
    "        patience_window: int = 20,\n",
    "        num_genome_repeats_per_iteration: int = 3,\n",
    "        max_repeats_per_genome: int = 30,\n",
    "        explosion_factor: float = 1e6,\n",
    "        accuracy_checks_every: int = 10,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        base_model_builder: function(genome)->model where model implements the interface expected by ParticleSwarmOptimisation:\n",
    "            - randomize(weight_scale, bias_scale)\n",
    "            - to_vector()/from_vector()\n",
    "            - forward(X)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.base_model_builder = base_model_builder\n",
    "        self.loss_function = loss_function\n",
    "        self.max_train_seconds = max_train_seconds\n",
    "        self.patience_window = patience_window\n",
    "        self.num_genome_repeats_per_iteration = num_genome_repeats_per_iteration\n",
    "        self.explosion_factor = explosion_factor\n",
    "        self.accuracy_checks_every = accuracy_checks_every\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def evaluate(self, genome: PsoGenome) -> float:\n",
    "        \"\"\"\n",
    "        Returns scalar fitness. Lower is better.\n",
    "        Implements:\n",
    "         - time-limited training\n",
    "         - early stopping on recent-window no-improvement (penalise)\n",
    "         - explosion detection (penalise)\n",
    "        \"\"\"\n",
    "        key = str(genome)\n",
    "        if key in self.cache and self.cache[key]['repeats'] >= self.num_genome_repeats_per_iteration:\n",
    "            return self.cache[key]['acc']\n",
    "\n",
    "        # build model\n",
    "        model = self.base_model_builder(genome)\n",
    "\n",
    "        # build PSO with the genome's params\n",
    "        accel = genome.accel\n",
    "        accel_obj = AccelerationCoefficients(\n",
    "            inertia_weight=accel.inertia_weight,\n",
    "            cognitive_weight=accel.cognitive_weight,\n",
    "            social_weight=accel.social_weight,\n",
    "            global_best_weight=accel.global_best_weight,\n",
    "            jump_size=accel.jump_size,\n",
    "            max_velocity=accel.max_velocity,\n",
    "            max_position=accel.max_position\n",
    "        )\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        for _ in range(self.num_genome_repeats_per_iteration):\n",
    "            pso = ParticleSwarmOptimisation(\n",
    "                X=self.X, Y=self.Y,\n",
    "                swarm_size=genome.swarm_size,\n",
    "                accel_coeff=accel_obj,\n",
    "                num_informants=max(1, min(genome.num_informants, genome.swarm_size - 1)),\n",
    "                loss_function=self.loss_function,\n",
    "                particle_initial_position_scale=genome.particle_initial_position_scale,\n",
    "                model=model\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "            last_losses = []\n",
    "            try:\n",
    "                epoch = 0\n",
    "                # replace PSO.train loop with a time-aware training\n",
    "                pso.update_informants()\n",
    "                # compute an initial loss to detect explosion (if available)\n",
    "                # We'll compute first fitness properly\n",
    "                initial_fitness = pso.update_best_global(0)\n",
    "                while True:\n",
    "                    # check time limit\n",
    "                    if time.time() - start_time > self.max_train_seconds:\n",
    "                        break\n",
    "                    # iterate a small PSO step: velocities, positions, recompute bests\n",
    "                    pso.update_velocities()\n",
    "                    pso.update_positions()\n",
    "                    avg_fitness = pso.update_best_global(epoch)\n",
    "                    last_losses.append(pso.best_global_fitness)\n",
    "\n",
    "                    # explosion detection\n",
    "                    if avg_fitness > initial_fitness * self.explosion_factor:\n",
    "                        # heavy penalty\n",
    "                        if self.verbose:\n",
    "                            print(\"[eval] explosion detected. stopping early.\")\n",
    "                        accuracies.append(0.0)\n",
    "\n",
    "                    if epoch % self.accuracy_checks_every == 0:\n",
    "                        acc = pso.get_accuracy(self.X_test, self.Y_test)\n",
    "                        accuracies.append(acc)\n",
    "                        if self.verbose:\n",
    "                            print(f\"[eval] epoch {epoch} best fitness {pso.best_global_fitness:.6g} avg fitness {avg_fitness:.6g} accuracy {acc:.2f}%\")\n",
    "                    # early stopping: check last window\n",
    "                    if len(last_losses) > self.patience_window:\n",
    "                        # consider improvement if best decreased at least once in window\n",
    "                        window = last_losses[-self.patience_window:]\n",
    "                        if max(window) < window[0]:  # no improvement\n",
    "                            if self.verbose:\n",
    "                                print(f\"[eval] early stopping at epoch {epoch}\")\n",
    "                            accuracies.append(pso.get_accuracy(self.X_test, self.Y_test))\n",
    "                            break\n",
    "                    epoch += 1\n",
    "                # normal return: the best found\n",
    "                if self.verbose:\n",
    "                    print(f\"[eval] completed training epochs: {epoch}, best fitness: {pso.best_global_fitness:.6g}\")\n",
    "                acc = pso.get_accuracy(self.X_test, self.Y_test)\n",
    "                accuracies.append(acc)\n",
    "            except Exception as e:\n",
    "                # crash in training -> penalize heavily\n",
    "                if self.verbose:\n",
    "                    print(\"[eval] Exception during PSO eval:\", e)\n",
    "                accuracies.append(0.0)\n",
    "            \n",
    "        if key in self.cache:\n",
    "            self.cache[key]['repeats'] += self.num_genome_repeats_per_iteration\n",
    "        else:\n",
    "            self.cache[key] = {'repeats': self.num_genome_repeats_per_iteration, 'accuracies': []}\n",
    "        updated_accuracies = self.cache[key]['accuracies'] + accuracies\n",
    "        mean_accuracy = np.mean(updated_accuracies)\n",
    "        self.cache[key]['acc'] = mean_accuracy\n",
    "\n",
    "        return mean_accuracy\n",
    "\n",
    "# --- Genetic algorithm ----------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GeneticIndividual:\n",
    "    genome: PsoGenome\n",
    "    accuracy: float = float('inf')\n",
    "\n",
    "class GeneticPsoOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        evaluator: PsoEvaluator,\n",
    "        population_size: int = 20,\n",
    "        generations: int = 30,\n",
    "        mutation_rate: float = 0.1,\n",
    "        crossover_rate: float = 0.8,\n",
    "        elitism: int = 2,\n",
    "        tournament_k: int = 3,\n",
    "        parallel: bool = True,\n",
    "    ):\n",
    "        self.evaluator = evaluator\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.elitism = elitism\n",
    "        self.tournament_k = tournament_k\n",
    "        self.parallel = parallel\n",
    "        self.population: List[GeneticIndividual] = []\n",
    "\n",
    "    def initialize(self, seed_genome_factory: Callable[[], PsoGenome]):\n",
    "        self.population = [GeneticIndividual(seed_genome_factory()) for _ in range(self.population_size)]\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        # Evaluate all individuals (optionally in parallel)\n",
    "        if self.parallel:\n",
    "            with mp.Pool(max(1, mp.cpu_count() - 1)) as pool:\n",
    "                genomes = [ind.genome for ind in self.population]\n",
    "                results = pool.map(self.evaluator.evaluate, genomes)\n",
    "            for ind, f in zip(self.population, results):\n",
    "                ind.accuracy = f\n",
    "        else:\n",
    "            for ind in self.population:\n",
    "                ind.accuracy = self.evaluator.evaluate(ind.genome)\n",
    "\n",
    "    def tournament_select(self) -> PsoGenome:\n",
    "        contenders = random.sample(self.population, self.tournament_k)\n",
    "        best = max(contenders, key=lambda ind: ind.accuracy)\n",
    "        return best.genome.copy()\n",
    "\n",
    "    def step(self):\n",
    "        # create next generation\n",
    "        new_pop: List[GeneticIndividual] = []\n",
    "        # elitism: carry best individuals\n",
    "        sorted_pop = sorted(self.population, key=lambda ind: ind.accuracy, reverse=True)\n",
    "        for i in range(self.elitism):\n",
    "            new_pop.append(GeneticIndividual(sorted_pop[i].genome.copy(), sorted_pop[i].accuracy))\n",
    "        # fill rest\n",
    "        while len(new_pop) < self.population_size:\n",
    "            parent_a = self.tournament_select()\n",
    "            if random.random() < self.crossover_rate:\n",
    "                parent_b = self.tournament_select()\n",
    "                child_genome = PsoGenome.crossover(parent_a, parent_b, crossover_rate=self.crossover_rate)\n",
    "            else:\n",
    "                child_genome = parent_a.copy()\n",
    "            child_genome.mutate(self.mutation_rate, bounds=self._bounds())\n",
    "            new_pop.append(GeneticIndividual(child_genome))\n",
    "        self.population = new_pop\n",
    "\n",
    "    def run(self, seed_genome_factory: Callable[[], PsoGenome], verbose: bool = True):\n",
    "        self.initialize(seed_genome_factory)\n",
    "        self.evaluate_population()\n",
    "        best_history = []\n",
    "        for g in range(self.generations):\n",
    "            if verbose:\n",
    "                best = max(self.population, key=lambda ind: ind.accuracy)\n",
    "                avg_fitness = sum(ind.accuracy for ind in self.population) / len(self.population)\n",
    "                print(f\"[GA] gen {g:02d} best {best.accuracy:.6g} avg {avg_fitness:.6g}\")\n",
    "                with open(\"ga_pso_log.csv\", \"a\") as f:\n",
    "                    f.write(f\"{g},{best.accuracy},{avg_fitness}\\n\")\n",
    "            self.step()\n",
    "            self.evaluate_population()\n",
    "            best_history.append(min(self.population, key=lambda ind: ind.accuracy).accuracy)\n",
    "        # final best\n",
    "        best_ind = min(self.population, key=lambda ind: ind.accuracy)\n",
    "        return best_ind, best_history\n",
    "\n",
    "    def _bounds(self):\n",
    "        # central place for bounds used by mutate: feel free to expose\n",
    "        return {\n",
    "            'swarm_size': (4, 200),\n",
    "            'position_scale': (1e-4, 1.0),\n",
    "            'bias_scale': (1e-6, 1.0),\n",
    "            'accel_ranges': {\n",
    "                'inertia_weight': (0.0, 2.0),\n",
    "                'cognitive_weight': (0.0, 4.0),\n",
    "                'social_weight': (0.0, 4.0),\n",
    "                'global_best_weight': (0.0, 4.0),\n",
    "                'jump_size': (1e-4, 2.0),\n",
    "                'max_velocity': (1e-6, 10.0),\n",
    "                'max_position': (1e-3, 100.0),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca917c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of GeneticPsoOptimizer\n",
    "\n",
    "optimizer = GeneticPsoOptimizer(\n",
    "    evaluator=PsoEvaluator(\n",
    "        X=train_features.T,\n",
    "        Y=train_targets,\n",
    "        X_test=test_features.T,\n",
    "        Y_test=test_targets,\n",
    "        base_model_builder=lambda genome: Sequential(\n",
    "            *(\n",
    "                [Linear(size_input=train_features.shape[1], size_hidden=genome.ann_layers[0])] +\n",
    "                sum(\n",
    "                    ([ActivationReLU(), Linear(size_input=genome.ann_layers[i], size_hidden=genome.ann_layers[i+1])]\n",
    "                     for i in range(len(genome.ann_layers)-1)),\n",
    "                    []\n",
    "                ) +\n",
    "                [ActivationReLU(), Linear(size_input=genome.ann_layers[-1], size_hidden=1)]\n",
    "            ) if genome.ann_layers else\n",
    "            [\n",
    "                Linear(size_input=train_features.shape[1], size_hidden=32),\n",
    "                ActivationReLU(),\n",
    "                Linear(size_input=32, size_hidden=16),\n",
    "                ActivationReLU(),\n",
    "                Linear(size_input=16, size_hidden=1)\n",
    "            ]\n",
    "        ),\n",
    "        loss_function=mean_squared_error,\n",
    "        max_train_seconds=20.0,\n",
    "        patience_window=20,\n",
    "        num_genome_repeats_per_iteration=5,\n",
    "        max_repeats_per_genome=40,\n",
    "        explosion_factor=1e2,\n",
    "        accuracy_checks_every=10,\n",
    "        verbose=True\n",
    "    ),\n",
    "    population_size=20,\n",
    "    generations=50,\n",
    "    mutation_rate=0.2,\n",
    "    crossover_rate=0.7,\n",
    "    elitism=6,\n",
    "    tournament_k=3,\n",
    "    parallel=False\n",
    ")\n",
    "\n",
    "optimizer.run(\n",
    "    seed_genome_factory=lambda: PsoGenome(\n",
    "        swarm_size=random.randint(10, 200),\n",
    "        accel=AccelerationCoefficientsGenome(\n",
    "            inertia_weight=random.uniform(0.2, 0.9),\n",
    "            cognitive_weight=random.uniform(0.5, 3.0),\n",
    "            social_weight=random.uniform(0.2, 2.0),\n",
    "            global_best_weight=random.uniform(0.1, 2.0),\n",
    "            jump_size=random.uniform(0.01, 1.0),\n",
    "            max_velocity=random.uniform(0.001, 2.0),\n",
    "            max_position=random.uniform(0.1, 10.0)\n",
    "        ),\n",
    "        num_informants=random.randint(1, 10),\n",
    "        particle_initial_position_scale=(random.uniform(0.0001, 0.1), random.uniform(0.0001, 0.1)),\n",
    "        ann_layers=(32, 16),\n",
    "        ann_activation=\"relu\"\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save output to a file\n",
    "with open(\"genetic_pso_optimizer_result.txt\", \"w\") as f:\n",
    "    best_individual = max(optimizer.population, key=lambda ind: ind.accuracy)\n",
    "    f.write(f\"Best fitness: {best_individual.accuracy}\\n\")\n",
    "    f.write(f\"Best genome: {best_individual.genome}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e90e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
