# Genetic Algorithm for PSO Hyperparameter Optimization generated by ChatGPT
# It is designed to find the best genome (hyperparameters of the PSO) because they are hard to fine tune by hand

from __future__ import annotations
import time
import copy
import random
import math
from dataclasses import dataclass, field
from typing import Tuple, List, Optional, Callable, Any, Dict
import numpy as np
import multiprocessing as mp
from utils import mean_squared_error
from pso import ParticleSwarmOptimisation, AccelerationCoefficients
from sequential import Sequential
from linear import Linear
from activations import ActivationReLU

from data import load_data

# Genetic Algorithm for PSO Hyperparameter Optimization generated by ChatGPT
# It is designed to find the best genome (hyperparameters of the PSO) because they are hard to fine tune by hand

# --- Genome definition -----------------------------------------------------

@dataclass
class AccelerationCoefficientsGenome:
    inertia_weight: float
    cognitive_weight: float
    social_weight: float
    global_best_weight: float
    jump_size: float
    max_velocity: float
    max_position: float

    def mutate(self, mutation_rate: float, ranges: Dict[str, Tuple[float, float]]):
        # gaussian perturbation per field with chance
        for name in vars(self):
            if random.random() < mutation_rate:
                lo, hi = ranges[name]
                cur = getattr(self, name)
                # relative gaussian step
                step = (hi - lo) * 0.1
                new = cur + random.gauss(0, step)
                setattr(self, name, float(np.clip(new, lo, hi)))

    @staticmethod
    def crossover(a: 'AccelerationCoefficientsGenome', b: 'AccelerationCoefficientsGenome') -> 'AccelerationCoefficientsGenome':
        # uniform crossover
        out = {}
        for name in vars(a):
            out[name] = getattr(a, name) if random.random() < 0.5 else getattr(b, name)
        return AccelerationCoefficientsGenome(**out)

@dataclass
class PsoGenome:
    swarm_size: int
    accel: AccelerationCoefficientsGenome
    num_informants: int
    particle_initial_position_scale: Tuple[float, float]
    # optionally include model architecture: list of hidden layer sizes and activation names
    ann_layers: Optional[Tuple[int, ...]] = None
    ann_activation: str = "tanh"
    # keep a pointer/identifier to the loss function if desired (not serializable)
    # loss_function: Callable = field(default=None, repr=False)

    def copy(self) -> 'PsoGenome':
        return copy.deepcopy(self)

    def mutate(self, mutation_rate: float, bounds: dict):
        # mutate swarm size (discrete), num_informants (discrete), scales and accel
        if random.random() < mutation_rate:
            self.swarm_size = int(np.clip(self.swarm_size + random.randint(-4, 4), bounds['swarm_size'][0], bounds['swarm_size'][1]))
        if random.random() < mutation_rate:
            self.num_informants = int(np.clip(self.num_informants + random.randint(-2, 2), 1, max(1, self.swarm_size - 1)))
        if random.random() < mutation_rate:
            s0, s1 = self.particle_initial_position_scale
            s0 += random.gauss(0, (bounds['position_scale'][1] - bounds['position_scale'][0]) * 0.05)
            s1 += random.gauss(0, (bounds['bias_scale'][1] - bounds['bias_scale'][0]) * 0.05)
            self.particle_initial_position_scale = (float(np.clip(s0, bounds['position_scale'][0], bounds['position_scale'][1])),
                                                   float(np.clip(s1, bounds['bias_scale'][0], bounds['bias_scale'][1])))
        # mutate acceleration coeffs
        self.accel.mutate(mutation_rate, bounds['accel_ranges'])
        # mutate architecture (small chance)
        if self.ann_layers is not None and random.random() < mutation_rate:
            layers = list(self.ann_layers)
            if random.random() < 0.5 and len(layers) > 0:
                # tweak a layer size
                idx = random.randrange(len(layers))
                layers[idx] = int(np.clip(layers[idx] + random.randint(-8, 8), 1, 1024))
            else:
                # either add or remove layer
                if random.random() < 0.5 and len(layers) > 1:
                    layers.pop(random.randrange(len(layers)))
                else:
                    # add a small layer
                    insert_at = random.randrange(len(layers)+1)
                    layers.insert(insert_at, random.randint(1, 32))
            self.ann_layers = tuple(layers)

    @staticmethod
    def crossover(a: 'PsoGenome', b: 'PsoGenome', crossover_rate: float = 0.5) -> 'PsoGenome':
        # single-point for architecture, uniform for many numeric
        child = a.copy()
        # swarm size: average with some chance
        child.swarm_size = int((a.swarm_size if random.random() < 0.5 else b.swarm_size))
        child.num_informants = int((a.num_informants if random.random() < 0.5 else b.num_informants))
        child.particle_initial_position_scale = (a.particle_initial_position_scale if random.random() < 0.5 else b.particle_initial_position_scale)
        child.accel = AccelerationCoefficientsGenome.crossover(a.accel, b.accel)
        # architecture crossover (if both defined)
        if a.ann_layers and b.ann_layers:
            if random.random() < crossover_rate:
                # one-point crossover on layer lists
                la, lb = list(a.ann_layers), list(b.ann_layers)
                cut_a = random.randrange(len(la))
                cut_b = random.randrange(len(lb))
                new_layers = tuple(la[:cut_a] + lb[cut_b:])
                child.ann_layers = new_layers
        else:
            child.ann_layers = a.ann_layers or b.ann_layers
        return child

# --- Evaluator -------------------------------------------------------------

class PsoEvaluator:
    def __init__(
        self,
        X: np.ndarray,
        Y: np.ndarray,
        X_test: np.ndarray,
        Y_test: np.ndarray,
        base_model_builder: Callable[[PsoGenome], Any],
        loss_function: Callable[[np.ndarray, np.ndarray], float],
        max_train_seconds: float = 10.0,
        patience_window: int = 20,
        num_genome_repeats_per_iteration: int = 3,
        max_repeats_per_genome: int = 30,
        explosion_factor: float = 1e6,
        accuracy_checks_every: int = 10,
        verbose: bool = False
    ):
        """
        base_model_builder: function(genome)->model where model implements the interface expected by ParticleSwarmOptimisation:
            - randomize(weight_scale, bias_scale)
            - to_vector()/from_vector()
            - forward(X)
        """
        self.X = X
        self.Y = Y
        self.X_test = X_test
        self.Y_test = Y_test
        self.base_model_builder = base_model_builder
        self.loss_function = loss_function
        self.max_train_seconds = max_train_seconds
        self.patience_window = patience_window
        self.num_genome_repeats_per_iteration = num_genome_repeats_per_iteration
        self.max_repeats_per_genome = max_repeats_per_genome
        self.explosion_factor = explosion_factor
        self.accuracy_checks_every = accuracy_checks_every
        self.verbose = verbose
        
        self.cache: Dict[str, Dict[str, Any]] = {}

    def evaluate(self, genome: PsoGenome) -> float:
        """
        Returns scalar fitness. Lower is better.
        Implements:
         - time-limited training
         - early stopping on recent-window no-improvement (penalise)
         - explosion detection (penalise)
        """
        key = str(genome)
        if key in self.cache and self.cache[key]['repeats'] >= self.max_repeats_per_genome:
            return self.cache[key]['acc']

        # build model
        model = self.base_model_builder(genome)

        # build PSO with the genome's params
        accel = genome.accel
        accel_obj = AccelerationCoefficients(
            inertia_weight=accel.inertia_weight,
            cognitive_weight=accel.cognitive_weight,
            social_weight=accel.social_weight,
            global_best_weight=accel.global_best_weight,
            jump_size=accel.jump_size,
            max_velocity=accel.max_velocity,
            max_position=accel.max_position
        )
        
        accuracies = []
        
        for _ in range(self.num_genome_repeats_per_iteration):
            pso = ParticleSwarmOptimisation(
                X=self.X, Y=self.Y,
                swarm_size=genome.swarm_size,
                accel_coeff=accel_obj,
                num_informants=max(1, min(genome.num_informants, genome.swarm_size - 1)),
                loss_function=self.loss_function,
                particle_initial_position_scale=genome.particle_initial_position_scale,
                model=model
            )

            start_time = time.time()
            last_losses = []
            try:
                epoch = 0
                # replace PSO.train loop with a time-aware training
                pso.update_informants()
                # compute an initial loss to detect explosion (if available)
                # We'll compute first fitness properly
                initial_fitness = pso.update_best_global()
                while True:
                    # check time limit
                    if time.time() - start_time > self.max_train_seconds:
                        break
                    # iterate a small PSO step: velocities, positions, recompute bests
                    pso.update_velocities()
                    pso.update_positions()
                    avg_fitness = pso.update_best_global()
                    last_losses.append(pso.best_global_fitness)

                    # explosion detection
                    if avg_fitness > initial_fitness * self.explosion_factor:
                        # heavy penalty
                        if self.verbose:
                            print("[eval] explosion detected. stopping early.")
                        accuracies.append(0.0)
                        break

                    if epoch % self.accuracy_checks_every == 0:
                        acc = pso.get_accuracy(self.X_test, self.Y_test)
                        accuracies.append(acc)
                    # early stopping: check last window
                    if len(last_losses) > self.patience_window:
                        # consider improvement if best decreased at least once in window
                        window = last_losses[-self.patience_window:]
                        if max(window) < window[0]:  # no improvement
                            if self.verbose:
                                print(f"[eval] early stopping at epoch {epoch}")
                            accuracies.append(pso.get_accuracy(self.X_test, self.Y_test))
                            break
                    epoch += 1
                # normal return: the best found
                acc = pso.get_accuracy(self.X_test, self.Y_test)
                if self.verbose:
                    print(f"[eval] completed training epochs: {epoch}, accuracy: {acc:.6g}")
                accuracies.append(acc)
            except Exception as e:
                # crash in training -> penalize heavily
                if self.verbose:
                    print("[eval] Exception during PSO eval:", e)
                accuracies.append(0.0)
            
        if key in self.cache:
            self.cache[key]['repeats'] += self.num_genome_repeats_per_iteration
        else:
            self.cache[key] = {'repeats': self.num_genome_repeats_per_iteration, 'accuracies': []}
        updated_accuracies = self.cache[key]['accuracies'] + accuracies
        mean_accuracy = np.mean(updated_accuracies)
        self.cache[key]['acc'] = mean_accuracy

        return mean_accuracy

# --- Genetic algorithm ----------------------------------------------------

@dataclass
class GeneticIndividual:
    genome: PsoGenome
    accuracy: float = float('inf')

class GeneticPsoOptimizer:
    def __init__(
        self,
        evaluator: PsoEvaluator,
        population_size: int = 20,
        generations: int = 30,
        mutation_rate: float = 0.1,
        crossover_rate: float = 0.8,
        elitism: int = 2,
        tournament_k: int = 3,
        parallel: bool = True,
    ):
        self.evaluator = evaluator
        self.population_size = population_size
        self.generations = generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elitism = elitism
        self.tournament_k = tournament_k
        self.parallel = parallel
        self.population: List[GeneticIndividual] = []

    def initialize(self, seed_genome_factory: Callable[[], PsoGenome]):
        self.population = [GeneticIndividual(seed_genome_factory()) for _ in range(self.population_size)]

    def evaluate_population(self):
        # Evaluate all individuals (optionally in parallel)
        if self.parallel:
            with mp.Pool(max(1, mp.cpu_count() - 1)) as pool:
                genomes = [ind.genome for ind in self.population]
                results = pool.map(self.evaluator.evaluate, genomes)
            for ind, f in zip(self.population, results):
                ind.accuracy = f
        else:
            for ind in self.population:
                ind.accuracy = self.evaluator.evaluate(ind.genome)

    def tournament_select(self) -> PsoGenome:
        contenders = random.sample(self.population, self.tournament_k)
        best = max(contenders, key=lambda ind: ind.accuracy)
        return best.genome.copy()

    def step(self):
        # create next generation
        new_pop: List[GeneticIndividual] = []
        # elitism: carry best individuals
        sorted_pop = sorted(self.population, key=lambda ind: ind.accuracy, reverse=True)
        for i in range(self.elitism):
            new_pop.append(GeneticIndividual(sorted_pop[i].genome.copy(), sorted_pop[i].accuracy))
        # fill rest
        while len(new_pop) < self.population_size:
            parent_a = self.tournament_select()
            if random.random() < self.crossover_rate:
                parent_b = self.tournament_select()
                child_genome = PsoGenome.crossover(parent_a, parent_b, crossover_rate=self.crossover_rate)
            else:
                child_genome = parent_a.copy()
            child_genome.mutate(self.mutation_rate, bounds=self._bounds())
            new_pop.append(GeneticIndividual(child_genome))
        self.population = new_pop

    def run(self, seed_genome_factory: Callable[[], PsoGenome], verbose: bool = True):
        self.initialize(seed_genome_factory)
        self.evaluate_population()
        best_history = []
        for g in range(self.generations):
            if verbose:
                best = max(self.population, key=lambda ind: ind.accuracy)
                avg_fitness = sum(ind.accuracy for ind in self.population) / len(self.population)
                print(f"[GA] gen {g:02d} best {best.accuracy:.6g} avg {avg_fitness:.6g}")
                with open("ga_pso_log.csv", "a") as f:
                    f.write(f"{g},{best.accuracy},{avg_fitness}\n")
            self.step()
            self.evaluate_population()
            best_history.append(min(self.population, key=lambda ind: ind.accuracy).accuracy)
        # final best
        best_ind = min(self.population, key=lambda ind: ind.accuracy)
        return best_ind, best_history

    def _bounds(self):
        # central place for bounds used by mutate: feel free to expose
        return {
            'swarm_size': (4, 200),
            'position_scale': (1e-4, 1.0),
            'bias_scale': (1e-6, 1.0),
            'accel_ranges': {
                'inertia_weight': (0.0, 2.0),
                'cognitive_weight': (0.0, 4.0),
                'social_weight': (0.0, 4.0),
                'global_best_weight': (0.0, 4.0),
                'jump_size': (1e-4, 2.0),
                'max_velocity': (1e-6, 10.0),
                'max_position': (1e-3, 100.0),
            }
        }


if __name__ == "__main__":
    (train_features, train_targets), (test_features, test_targets) = load_data()

    # Example usage of GeneticPsoOptimizer
    optimizer = GeneticPsoOptimizer(
        evaluator=PsoEvaluator(
            X=train_features.T,
            Y=train_targets,
            base_model_builder=lambda genome: Sequential(
                *(
                    [Linear(size_input=train_features.shape[1], size_hidden=genome.ann_layers[0])] +
                    sum(
                        ([ActivationReLU(), Linear(size_input=genome.ann_layers[i], size_hidden=genome.ann_layers[i+1])]
                        for i in range(len(genome.ann_layers)-1)),
                        []
                    ) +
                    [ActivationReLU(), Linear(size_input=genome.ann_layers[-1], size_hidden=1)]
                ) if genome.ann_layers else
                [
                    Linear(size_input=train_features.shape[1], size_hidden=32),
                    ActivationReLU(),
                    Linear(size_input=32, size_hidden=16),
                    ActivationReLU(),
                    Linear(size_input=16, size_hidden=1)
                ]
            ),
            loss_function=mean_squared_error,
            max_train_seconds=10.0,
            patience_window=10,
            explosion_factor=1e1,
            verbose=False
        ),
        population_size=20,
        generations=30,
        mutation_rate=0.2,
        crossover_rate=0.7,
        elitism=5,
        tournament_k=3,
        parallel=False
    )

    optimizer.run(
        seed_genome_factory=lambda: PsoGenome(
            swarm_size=random.randint(10, 200),
            accel=AccelerationCoefficientsGenome(
                inertia_weight=random.uniform(0.4, 0.9),
                cognitive_weight=random.uniform(1.0, 3.0),
                social_weight=random.uniform(0.5, 2.0),
                global_best_weight=random.uniform(0.1, 1.0),
                jump_size=random.uniform(0.01, 1.0),
                max_velocity=random.uniform(0.001, 1.0),
                max_position=random.uniform(0.1, 10.0)
            ),
            num_informants=random.randint(1, 10),
            particle_initial_position_scale=(random.uniform(0.0001, 0.1), random.uniform(0.0001, 0.1)),
            ann_layers=(32, 16),
            ann_activation="relu"
        ),
        verbose=True
    )

    # Save output to a file
    with open("genetic_pso_optimizer_result.txt", "w") as f:
        best_individual = min(optimizer.population, key=lambda ind: ind.fitness)
        f.write(f"Best fitness: {best_individual.fitness}\n")
        f.write(f"Best genome: {best_individual.genome}\n")
    import pickle
    with open("genetic_pso_optimizer_result.pkl", "wb") as f:
        pickle.dump(optimizer, f)
